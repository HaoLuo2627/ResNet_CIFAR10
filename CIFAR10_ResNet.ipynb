{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOnnUwu91VUzp/Fsp3FqrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyrankhademi/ResNet_CIFAR10/blob/master/CIFAR10_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fe37K5CtO0g",
        "colab_type": "text"
      },
      "source": [
        "#Reproducing CIFAR10 Experiment in the ResNet paper\n",
        "This Ipython notebook is written by Seyran Khademi as an example document for the reproducibility project in deep learning course CS4180 at Delft University of Technology. In this notebook we reproduce Table 6 in [1] , i.e. the CIFAR10 experiment in the original ResNet paper published in CVPR 2016 conference and received more than 38k citation so far. This Pytorch implementation is based on adaptation of the code in [2] and [3] to Jupyter notebook. \n",
        "\n",
        "## How/Why ResNet Models are Working?\n",
        "There has been rigorous attemps to make deeper convolutional neural networks (CNN) since their advent in 2012 [4] as the performance is believed to be tightly related to the complexity of the network [5]. A major obstacle in training deeper neural networks is the well-known \"vanishing gradient\" problem. As the layers are added to the network the multiplying gradients makes the overall gradient infinitesimal which in turn causes very slow convergence if at all [6]. The same training difficulties with \"exploding gradients\" can also hinder the learning process once layers are stacked in the neural networks. In 2015, a novel CNN architecture is introduced, which won the ImageNet classification competition (ILSVRC 2015) by a good margin (2.84 %) from its competitor [7]. \n",
        "\n",
        "The intuition behind the ResNet architecture is rather simple: Assuming that a neural network unit can learn any function, asymptotically, then it can learn the identity function as well. An example residual unit is shown in the following figure, taken from [1]. The input to the residual block is $X$ and the output is $\\mathcal{F}(X)+X$, therefore, by learning $\\mathcal{F}(X)=0$ this basic block is bypassed during the training process, which is equivalent to identity mapping. A cascade of these residual blocks are used to create very deep CNN models with more than 100 layers as presented in [1].  \n",
        "\n",
        "![](https://drive.google.com/uc?id=1c4QvJN4H_GdGWNM-vW46j_JIG64CD_mD)\n",
        "\n",
        "Note that a plain CNN model (without residual connections) posses the same solution space as the counterpart network with the residual connections, however it is argued in [1] that \"If the optimal function is closer to an identity\n",
        "mapping than to a zero mapping, it should be easier for the\n",
        "solver to find the perturbations with reference to an identity\n",
        "mapping, than to learn the function as a new one.\" This hypothesis is backed up the paper with several experiments in different datasets. \n",
        "\n",
        "## Experiment Setup \n",
        "The authors train and test six different ResNet architectures for CIFAR10 dataset [8] and compare the results on Table 6 in the original paper. CIFAR10 image classification dataset consist of 50k training images and 10k testing\n",
        "images in 10 classes. The network inputs are $32\u0002\\times 32$ images, with\n",
        "the per-pixel mean subtracted. The first layer is $3\u0002\\times 3$ convolutions.\n",
        "There are totally $6n+2$ residual blocks stacked, where replacing $n$ with $3,5,7,9,18,200$ produces networks of depth $20,32,44,56,110,1202$, respectively. The architecture is summerized in the following table taken from the paper, where three columns represent three different feature-map size. \n",
        "\n",
        "![](https://drive.google.com/uc?id=1W_k5HZ8lS9_h9BUOx9R0AvZPTqRVDSlT)\n",
        "\n",
        "For the rest, we use the same setting as described in the original paper and we reproduce the following results: \n",
        "\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, \"Deep Residual Learning for Image Recognition\". CVPR 2016\n",
        "\n",
        "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "\n",
        "[3] https://github.com/akamaster/pytorch_resnet_cifar10\n",
        "\n",
        "[4] Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, \"ImageNet Classification with Deep Convolutional Neural Networks\", NIPS 2012\n",
        "\n",
        "[5] K. Simonyan and A. Zisserman,\"Very deep convolutional networks\n",
        "for large-scale image recognition\" ICLR 2015\n",
        "\n",
        "[6] X. Glorot and Y. Bengio, \"Understanding the difficulty of training\n",
        "deep feedforward neural networks\", AISTATS 2010\n",
        "\n",
        "[7] K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers:\n",
        "Surpassing human-level performance on imagenet classification\", ICCV 2015\n",
        "\n",
        "[8] https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAEdiF4Ymizi",
        "colab_type": "text"
      },
      "source": [
        "## Running the Experiment on Google Colab\n",
        "This notebook is running remotly on Google Colab platform, therefore to save and access the trained model and chekpoints in your local computer you may need to mounth the Google drive (gdrive).  The following code snippet will setup a local drive in your computer. Then, you can create a directory in your gdrive for this project and specify the path to this directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H418uCYl6OlQ",
        "colab_type": "code",
        "outputId": "b6637e10-539e-4ecd-c548-22150983111d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# First we need to mount the Google drive and specify the path to the directory\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/CIFAR10_ResNet\" \n",
        "root_path = 'gdrive/My Drive/CIFAR10_ResNet' \n",
        "path ='/content/gdrive/My Drive/CIFAR10_ResNet'\n",
        "os.chdir(path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "data\t\t   __pycache__\tresnet.py  save_temp  trainer.py\n",
            "pretrained_models  README.md\trun.sh\t   test.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjvbl18HsjnT",
        "colab_type": "text"
      },
      "source": [
        "In the following code cell, the ResNet model is defined with all the related functions for initialization of the weights, the residual block and skip connections as presented in the original paper. By executing the cell you can see all possible ResNet architecture, with the number of their learning parameters and layers, that we use in this experiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Y2hYRwB-qg",
        "colab_type": "code",
        "outputId": "ab579bbf-a511-41d0-9c91-f57e79c60782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import resnet\n",
        "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        " \n",
        "def _weights_init(m):\n",
        "    \"\"\"\n",
        "        Initialization of CNN weights\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
        "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
        "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
        "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 experiment, ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
        "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
        "# The subsampling is performed by convolutions with a stride of 2.\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resnet20\n",
            "Total number of params 269722\n",
            "Total layers 20\n",
            "\n",
            "resnet32\n",
            "Total number of params 464154\n",
            "Total layers 32\n",
            "\n",
            "resnet44\n",
            "Total number of params 658586\n",
            "Total layers 44\n",
            "\n",
            "resnet56\n",
            "Total number of params 853018\n",
            "Total layers 56\n",
            "\n",
            "resnet110\n",
            "Total number of params 1727962\n",
            "Total layers 110\n",
            "\n",
            "resnet1202\n",
            "Total number of params 19421274\n",
            "Total layers 1202\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRMm1zq2J7a",
        "colab_type": "text"
      },
      "source": [
        "We define a class (MyResNetArgs) in the following to assign the hyperparameters such as number of training epochs, learning rate, momentum, batch size, etc. to the training function. The objects of this class are initialized inherently once created with empty argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXmCsEZ6dFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class MyResNetArgs:\n",
        "   \"\"\"\n",
        "    Passing the hyperparameters to the model\n",
        "   \"\"\"\n",
        "   def __init__(self, arch='resnet20' ,epochs=200, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
        "                 evaluate=0, half=0, save_dir='save_temp', save_every=10):\n",
        "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir #The directory used to save the trained models\n",
        "        self.half = half #use half-precision(16-bit)\n",
        "        self.evaluate = evaluate #evaluate model on validation set\n",
        "        self.print_freq = print_freq #print frequency \n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum \n",
        "        self.lr = lr #Learning rate\n",
        "        self.batch_size = batch_size \n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.arch = arch #ResNet model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNzFe-13pWj",
        "colab_type": "text"
      },
      "source": [
        "Now we can create an instance of ResNet model and inspect the architecture by printing the model summery. \n",
        "One can easily check the difference between different ResNet models to understand the constructing units. There are totally $6n+2$ stacked weighted layers, e.g., for ResNet 20, there are 19 convolutional layers plus one fully connected layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djW0RO80_prY",
        "colab_type": "code",
        "outputId": "54e818e3-8cb5-4614-cc52-8dcd7222a5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "args=MyResNetArgs('resnet110',evaluate=0,epochs=200)\n",
        "#model = resnet.__dict__[args.arch]()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "model = resnet.__dict__[args.arch]().to(device)\n",
        "summary(model, (3,32,32))\n",
        "best_prec1 = 0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
            "        BasicBlock-7           [-1, 16, 32, 32]               0\n",
            "            Conv2d-8           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-9           [-1, 16, 32, 32]              32\n",
            "           Conv2d-10           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-12           [-1, 16, 32, 32]               0\n",
            "           Conv2d-13           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
            "           Conv2d-20           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-21           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-22           [-1, 16, 32, 32]               0\n",
            "           Conv2d-23           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-24           [-1, 16, 32, 32]              32\n",
            "           Conv2d-25           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-26           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-27           [-1, 16, 32, 32]               0\n",
            "           Conv2d-28           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-29           [-1, 16, 32, 32]              32\n",
            "           Conv2d-30           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-31           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-32           [-1, 16, 32, 32]               0\n",
            "           Conv2d-33           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-34           [-1, 16, 32, 32]              32\n",
            "           Conv2d-35           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-36           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-37           [-1, 16, 32, 32]               0\n",
            "           Conv2d-38           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-39           [-1, 16, 32, 32]              32\n",
            "           Conv2d-40           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-41           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-42           [-1, 16, 32, 32]               0\n",
            "           Conv2d-43           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-44           [-1, 16, 32, 32]              32\n",
            "           Conv2d-45           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-46           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-47           [-1, 16, 32, 32]               0\n",
            "           Conv2d-48           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-49           [-1, 16, 32, 32]              32\n",
            "           Conv2d-50           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-51           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-52           [-1, 16, 32, 32]               0\n",
            "           Conv2d-53           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-54           [-1, 16, 32, 32]              32\n",
            "           Conv2d-55           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-56           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-57           [-1, 16, 32, 32]               0\n",
            "           Conv2d-58           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-59           [-1, 16, 32, 32]              32\n",
            "           Conv2d-60           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-61           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-62           [-1, 16, 32, 32]               0\n",
            "           Conv2d-63           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-64           [-1, 16, 32, 32]              32\n",
            "           Conv2d-65           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-66           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-67           [-1, 16, 32, 32]               0\n",
            "           Conv2d-68           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-69           [-1, 16, 32, 32]              32\n",
            "           Conv2d-70           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-71           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-72           [-1, 16, 32, 32]               0\n",
            "           Conv2d-73           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-74           [-1, 16, 32, 32]              32\n",
            "           Conv2d-75           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-76           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-77           [-1, 16, 32, 32]               0\n",
            "           Conv2d-78           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-79           [-1, 16, 32, 32]              32\n",
            "           Conv2d-80           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-81           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-82           [-1, 16, 32, 32]               0\n",
            "           Conv2d-83           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-84           [-1, 16, 32, 32]              32\n",
            "           Conv2d-85           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-86           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-87           [-1, 16, 32, 32]               0\n",
            "           Conv2d-88           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-89           [-1, 16, 32, 32]              32\n",
            "           Conv2d-90           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-91           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-92           [-1, 16, 32, 32]               0\n",
            "           Conv2d-93           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-94           [-1, 32, 16, 16]              64\n",
            "           Conv2d-95           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-96           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-97           [-1, 32, 16, 16]               0\n",
            "       BasicBlock-98           [-1, 32, 16, 16]               0\n",
            "           Conv2d-99           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-100           [-1, 32, 16, 16]              64\n",
            "          Conv2d-101           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-102           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-103           [-1, 32, 16, 16]               0\n",
            "          Conv2d-104           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-105           [-1, 32, 16, 16]              64\n",
            "          Conv2d-106           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-107           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-108           [-1, 32, 16, 16]               0\n",
            "          Conv2d-109           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-110           [-1, 32, 16, 16]              64\n",
            "          Conv2d-111           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-112           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-113           [-1, 32, 16, 16]               0\n",
            "          Conv2d-114           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-115           [-1, 32, 16, 16]              64\n",
            "          Conv2d-116           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-117           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-118           [-1, 32, 16, 16]               0\n",
            "          Conv2d-119           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-120           [-1, 32, 16, 16]              64\n",
            "          Conv2d-121           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-122           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-123           [-1, 32, 16, 16]               0\n",
            "          Conv2d-124           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-125           [-1, 32, 16, 16]              64\n",
            "          Conv2d-126           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-127           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-128           [-1, 32, 16, 16]               0\n",
            "          Conv2d-129           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-130           [-1, 32, 16, 16]              64\n",
            "          Conv2d-131           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-132           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-133           [-1, 32, 16, 16]               0\n",
            "          Conv2d-134           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-135           [-1, 32, 16, 16]              64\n",
            "          Conv2d-136           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-137           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-138           [-1, 32, 16, 16]               0\n",
            "          Conv2d-139           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-140           [-1, 32, 16, 16]              64\n",
            "          Conv2d-141           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-142           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-143           [-1, 32, 16, 16]               0\n",
            "          Conv2d-144           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-145           [-1, 32, 16, 16]              64\n",
            "          Conv2d-146           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-147           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-148           [-1, 32, 16, 16]               0\n",
            "          Conv2d-149           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-150           [-1, 32, 16, 16]              64\n",
            "          Conv2d-151           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-152           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-153           [-1, 32, 16, 16]               0\n",
            "          Conv2d-154           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-155           [-1, 32, 16, 16]              64\n",
            "          Conv2d-156           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-157           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-158           [-1, 32, 16, 16]               0\n",
            "          Conv2d-159           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-160           [-1, 32, 16, 16]              64\n",
            "          Conv2d-161           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-162           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-163           [-1, 32, 16, 16]               0\n",
            "          Conv2d-164           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-165           [-1, 32, 16, 16]              64\n",
            "          Conv2d-166           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-167           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-168           [-1, 32, 16, 16]               0\n",
            "          Conv2d-169           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-170           [-1, 32, 16, 16]              64\n",
            "          Conv2d-171           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-172           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-173           [-1, 32, 16, 16]               0\n",
            "          Conv2d-174           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-175           [-1, 32, 16, 16]              64\n",
            "          Conv2d-176           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-177           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-178           [-1, 32, 16, 16]               0\n",
            "          Conv2d-179           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-180           [-1, 32, 16, 16]              64\n",
            "          Conv2d-181           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-182           [-1, 32, 16, 16]              64\n",
            "      BasicBlock-183           [-1, 32, 16, 16]               0\n",
            "          Conv2d-184             [-1, 64, 8, 8]          18,432\n",
            "     BatchNorm2d-185             [-1, 64, 8, 8]             128\n",
            "          Conv2d-186             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-187             [-1, 64, 8, 8]             128\n",
            "     LambdaLayer-188             [-1, 64, 8, 8]               0\n",
            "      BasicBlock-189             [-1, 64, 8, 8]               0\n",
            "          Conv2d-190             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-191             [-1, 64, 8, 8]             128\n",
            "          Conv2d-192             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-193             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-194             [-1, 64, 8, 8]               0\n",
            "          Conv2d-195             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-196             [-1, 64, 8, 8]             128\n",
            "          Conv2d-197             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-198             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-199             [-1, 64, 8, 8]               0\n",
            "          Conv2d-200             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-201             [-1, 64, 8, 8]             128\n",
            "          Conv2d-202             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-203             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-204             [-1, 64, 8, 8]               0\n",
            "          Conv2d-205             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-206             [-1, 64, 8, 8]             128\n",
            "          Conv2d-207             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-208             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-209             [-1, 64, 8, 8]               0\n",
            "          Conv2d-210             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-211             [-1, 64, 8, 8]             128\n",
            "          Conv2d-212             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-213             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-214             [-1, 64, 8, 8]               0\n",
            "          Conv2d-215             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-216             [-1, 64, 8, 8]             128\n",
            "          Conv2d-217             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-218             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-219             [-1, 64, 8, 8]               0\n",
            "          Conv2d-220             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-221             [-1, 64, 8, 8]             128\n",
            "          Conv2d-222             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-223             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-224             [-1, 64, 8, 8]               0\n",
            "          Conv2d-225             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-226             [-1, 64, 8, 8]             128\n",
            "          Conv2d-227             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-228             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-229             [-1, 64, 8, 8]               0\n",
            "          Conv2d-230             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-231             [-1, 64, 8, 8]             128\n",
            "          Conv2d-232             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-233             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-234             [-1, 64, 8, 8]               0\n",
            "          Conv2d-235             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-236             [-1, 64, 8, 8]             128\n",
            "          Conv2d-237             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-238             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-239             [-1, 64, 8, 8]               0\n",
            "          Conv2d-240             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-241             [-1, 64, 8, 8]             128\n",
            "          Conv2d-242             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-243             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-244             [-1, 64, 8, 8]               0\n",
            "          Conv2d-245             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-246             [-1, 64, 8, 8]             128\n",
            "          Conv2d-247             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-248             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-249             [-1, 64, 8, 8]               0\n",
            "          Conv2d-250             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-251             [-1, 64, 8, 8]             128\n",
            "          Conv2d-252             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-253             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-254             [-1, 64, 8, 8]               0\n",
            "          Conv2d-255             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-256             [-1, 64, 8, 8]             128\n",
            "          Conv2d-257             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-258             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-259             [-1, 64, 8, 8]               0\n",
            "          Conv2d-260             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-261             [-1, 64, 8, 8]             128\n",
            "          Conv2d-262             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-263             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-264             [-1, 64, 8, 8]               0\n",
            "          Conv2d-265             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-266             [-1, 64, 8, 8]             128\n",
            "          Conv2d-267             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-268             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-269             [-1, 64, 8, 8]               0\n",
            "          Conv2d-270             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-271             [-1, 64, 8, 8]             128\n",
            "          Conv2d-272             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-273             [-1, 64, 8, 8]             128\n",
            "      BasicBlock-274             [-1, 64, 8, 8]               0\n",
            "          Linear-275                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 1,727,962\n",
            "Trainable params: 1,727,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 20.03\n",
            "Params size (MB): 6.59\n",
            "Estimated Total Size (MB): 26.63\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAb3r6OQ43ve",
        "colab_type": "text"
      },
      "source": [
        "The next two code blocks are for training and testing the model on the validation set. There is a print module at the end of train function which prints the top-k classification accuracy and error at specified epochs which is set in \"print_freq\" hyper-parameter. The checkpoints are also saved for the training model at \"save_every\" epoch steps, which is initialized to every 10 epochs by default.  The average accuracy among mini-batches are also recorded for inspection purposes, which is calculated using the \"AverageMeter\" function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTjp-tkWtmmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "        if args.half:\n",
        "            input_var = input_var.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B57y5hgMtzDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            if args.half:\n",
        "                input_var = input_var.half()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKt6Q9Zlt8MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKFeRNm7mJA",
        "colab_type": "text"
      },
      "source": [
        "Here comes the main script that we need to run to execute the training and the test procedure for a specified ResNet model. Since, Google Colab imposes time restriction, we can not loop over all ResNet models to reproduce the results in one go, instead, we run each ResNet model seperatly by setting the name of the model (e.g. resnet20)  manually in the hyperparameters and record the results once training is finished for the specified network. We saved the trained models in a seperate directory to do the inference later. One can try to run the main script several times for each network and report the mean and variance performance for more stable results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONRZVaZtP0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1\n",
        "    \n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    model = resnet.__dict__[args.arch]()\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    if args.half:\n",
        "        print('half persicion is used.')\n",
        "        model.half()\n",
        "        criterion.half()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
        "\n",
        "    if args.arch in ['resnet1202', 'resnet110']:\n",
        "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
        "        # then switch back. In this setup it will correspond for first epoch.\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = args.lr*0.1\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        print('evalution mode')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('Training {} model'.format(args.arch))\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
        "\n",
        "    return best_prec1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkntiMU2uKk6",
        "colab_type": "code",
        "outputId": "9fe3544a-5014-4416-9ab6-a70399b08e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   best_prec1 = main()\n",
        "   print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [0][0/391]\tLoss 5.6544 (5.6544)\tPrec@1 10.938 (10.938)\n",
            "Epoch: [0][55/391]\tLoss 1.9227 (2.6400)\tPrec@1 30.469 (20.215)\n",
            "Epoch: [0][110/391]\tLoss 1.7225 (2.2721)\tPrec@1 31.250 (24.768)\n",
            "Epoch: [0][165/391]\tLoss 1.6063 (2.1083)\tPrec@1 35.938 (27.400)\n",
            "Epoch: [0][220/391]\tLoss 1.4614 (1.9971)\tPrec@1 42.969 (30.090)\n",
            "Epoch: [0][275/391]\tLoss 1.5646 (1.9157)\tPrec@1 40.625 (32.311)\n",
            "Epoch: [0][330/391]\tLoss 1.4815 (1.8547)\tPrec@1 47.656 (34.174)\n",
            "Epoch: [0][385/391]\tLoss 1.3680 (1.8003)\tPrec@1 50.000 (35.810)\n",
            "Test\t  Prec@1: 46.700 (Err: 53.300 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [1][0/391]\tLoss 1.4504 (1.4504)\tPrec@1 48.438 (48.438)\n",
            "Epoch: [1][55/391]\tLoss 1.4347 (1.4309)\tPrec@1 48.438 (47.112)\n",
            "Epoch: [1][110/391]\tLoss 1.3791 (1.4230)\tPrec@1 50.781 (47.713)\n",
            "Epoch: [1][165/391]\tLoss 1.3561 (1.4063)\tPrec@1 47.656 (48.362)\n",
            "Epoch: [1][220/391]\tLoss 1.2025 (1.3845)\tPrec@1 59.375 (49.314)\n",
            "Epoch: [1][275/391]\tLoss 1.3754 (1.3717)\tPrec@1 52.344 (49.856)\n",
            "Epoch: [1][330/391]\tLoss 1.1952 (1.3557)\tPrec@1 50.781 (50.524)\n",
            "Epoch: [1][385/391]\tLoss 1.1924 (1.3408)\tPrec@1 60.938 (51.099)\n",
            "Test\t  Prec@1: 58.190 (Err: 41.810 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [2][0/391]\tLoss 1.2589 (1.2589)\tPrec@1 56.250 (56.250)\n",
            "Epoch: [2][55/391]\tLoss 1.0835 (1.2367)\tPrec@1 57.031 (54.436)\n",
            "Epoch: [2][110/391]\tLoss 1.3680 (1.2144)\tPrec@1 48.438 (55.912)\n",
            "Epoch: [2][165/391]\tLoss 1.0500 (1.1926)\tPrec@1 65.625 (56.857)\n",
            "Epoch: [2][220/391]\tLoss 1.1984 (1.1754)\tPrec@1 58.594 (57.618)\n",
            "Epoch: [2][275/391]\tLoss 0.9650 (1.1627)\tPrec@1 63.281 (58.070)\n",
            "Epoch: [2][330/391]\tLoss 1.1702 (1.1495)\tPrec@1 59.375 (58.495)\n",
            "Epoch: [2][385/391]\tLoss 1.3190 (1.1415)\tPrec@1 57.812 (58.903)\n",
            "Test\t  Prec@1: 62.030 (Err: 37.970 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [3][0/391]\tLoss 0.8296 (0.8296)\tPrec@1 70.312 (70.312)\n",
            "Epoch: [3][55/391]\tLoss 0.9408 (1.0579)\tPrec@1 67.969 (62.319)\n",
            "Epoch: [3][110/391]\tLoss 0.9782 (1.0534)\tPrec@1 68.750 (62.190)\n",
            "Epoch: [3][165/391]\tLoss 0.9671 (1.0399)\tPrec@1 67.188 (62.872)\n",
            "Epoch: [3][220/391]\tLoss 0.9243 (1.0321)\tPrec@1 67.969 (63.182)\n",
            "Epoch: [3][275/391]\tLoss 0.9973 (1.0219)\tPrec@1 64.062 (63.723)\n",
            "Epoch: [3][330/391]\tLoss 0.8842 (1.0128)\tPrec@1 66.406 (63.942)\n",
            "Epoch: [3][385/391]\tLoss 0.9811 (1.0081)\tPrec@1 64.062 (64.123)\n",
            "Test\t  Prec@1: 66.320 (Err: 33.680 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [4][0/391]\tLoss 0.9159 (0.9159)\tPrec@1 67.188 (67.188)\n",
            "Epoch: [4][55/391]\tLoss 0.9785 (0.9145)\tPrec@1 63.281 (67.662)\n",
            "Epoch: [4][110/391]\tLoss 0.8904 (0.9161)\tPrec@1 72.656 (67.504)\n",
            "Epoch: [4][165/391]\tLoss 0.8724 (0.9107)\tPrec@1 74.219 (67.870)\n",
            "Epoch: [4][220/391]\tLoss 0.9323 (0.9129)\tPrec@1 69.531 (67.792)\n",
            "Epoch: [4][275/391]\tLoss 0.8487 (0.9045)\tPrec@1 68.750 (68.000)\n",
            "Epoch: [4][330/391]\tLoss 0.8064 (0.9014)\tPrec@1 69.531 (68.096)\n",
            "Epoch: [4][385/391]\tLoss 0.8152 (0.8967)\tPrec@1 74.219 (68.278)\n",
            "Test\t  Prec@1: 70.250 (Err: 29.750 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [5][0/391]\tLoss 0.8764 (0.8764)\tPrec@1 68.750 (68.750)\n",
            "Epoch: [5][55/391]\tLoss 0.8648 (0.8466)\tPrec@1 63.281 (69.894)\n",
            "Epoch: [5][110/391]\tLoss 0.9290 (0.8427)\tPrec@1 67.188 (70.151)\n",
            "Epoch: [5][165/391]\tLoss 0.7814 (0.8411)\tPrec@1 69.531 (70.157)\n",
            "Epoch: [5][220/391]\tLoss 0.7165 (0.8352)\tPrec@1 76.562 (70.680)\n",
            "Epoch: [5][275/391]\tLoss 0.8233 (0.8282)\tPrec@1 68.750 (70.915)\n",
            "Epoch: [5][330/391]\tLoss 0.7476 (0.8229)\tPrec@1 75.000 (71.132)\n",
            "Epoch: [5][385/391]\tLoss 0.8023 (0.8210)\tPrec@1 69.531 (71.225)\n",
            "Test\t  Prec@1: 68.870 (Err: 31.130 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [6][0/391]\tLoss 0.6983 (0.6983)\tPrec@1 78.125 (78.125)\n",
            "Epoch: [6][55/391]\tLoss 0.5884 (0.7569)\tPrec@1 76.562 (73.493)\n",
            "Epoch: [6][110/391]\tLoss 0.6516 (0.7582)\tPrec@1 78.125 (73.290)\n",
            "Epoch: [6][165/391]\tLoss 0.9375 (0.7467)\tPrec@1 71.094 (73.696)\n",
            "Epoch: [6][220/391]\tLoss 0.8432 (0.7506)\tPrec@1 67.969 (73.653)\n",
            "Epoch: [6][275/391]\tLoss 0.7256 (0.7456)\tPrec@1 70.312 (73.961)\n",
            "Epoch: [6][330/391]\tLoss 0.8999 (0.7429)\tPrec@1 67.969 (74.075)\n",
            "Epoch: [6][385/391]\tLoss 0.8082 (0.7415)\tPrec@1 75.000 (74.099)\n",
            "Test\t  Prec@1: 70.570 (Err: 29.430 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [7][0/391]\tLoss 0.8123 (0.8123)\tPrec@1 73.438 (73.438)\n",
            "Epoch: [7][55/391]\tLoss 0.6223 (0.7028)\tPrec@1 78.906 (75.977)\n",
            "Epoch: [7][110/391]\tLoss 0.6475 (0.6895)\tPrec@1 76.562 (75.950)\n",
            "Epoch: [7][165/391]\tLoss 0.7865 (0.6890)\tPrec@1 72.656 (75.951)\n",
            "Epoch: [7][220/391]\tLoss 0.6765 (0.6876)\tPrec@1 71.875 (75.954)\n",
            "Epoch: [7][275/391]\tLoss 0.8113 (0.6868)\tPrec@1 70.312 (76.039)\n",
            "Epoch: [7][330/391]\tLoss 0.4849 (0.6821)\tPrec@1 82.812 (76.166)\n",
            "Epoch: [7][385/391]\tLoss 0.6087 (0.6825)\tPrec@1 77.344 (76.139)\n",
            "Test\t  Prec@1: 75.940 (Err: 24.060 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [8][0/391]\tLoss 0.5015 (0.5015)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [8][55/391]\tLoss 0.6664 (0.6344)\tPrec@1 81.250 (78.125)\n",
            "Epoch: [8][110/391]\tLoss 0.6789 (0.6468)\tPrec@1 78.125 (77.808)\n",
            "Epoch: [8][165/391]\tLoss 0.5940 (0.6497)\tPrec@1 82.031 (77.508)\n",
            "Epoch: [8][220/391]\tLoss 0.6644 (0.6491)\tPrec@1 78.125 (77.467)\n",
            "Epoch: [8][275/391]\tLoss 0.5248 (0.6464)\tPrec@1 82.031 (77.601)\n",
            "Epoch: [8][330/391]\tLoss 0.5509 (0.6435)\tPrec@1 82.031 (77.750)\n",
            "Epoch: [8][385/391]\tLoss 0.6168 (0.6396)\tPrec@1 80.469 (77.884)\n",
            "Test\t  Prec@1: 75.530 (Err: 24.470 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [9][0/391]\tLoss 0.5507 (0.5507)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [9][55/391]\tLoss 0.6075 (0.5925)\tPrec@1 78.125 (79.576)\n",
            "Epoch: [9][110/391]\tLoss 0.5957 (0.6008)\tPrec@1 79.688 (79.160)\n",
            "Epoch: [9][165/391]\tLoss 0.5251 (0.5936)\tPrec@1 81.250 (79.264)\n",
            "Epoch: [9][220/391]\tLoss 0.7696 (0.5965)\tPrec@1 75.000 (79.186)\n",
            "Epoch: [9][275/391]\tLoss 0.5103 (0.5930)\tPrec@1 83.594 (79.317)\n",
            "Epoch: [9][330/391]\tLoss 0.6266 (0.5943)\tPrec@1 78.906 (79.267)\n",
            "Epoch: [9][385/391]\tLoss 0.5914 (0.5953)\tPrec@1 77.344 (79.244)\n",
            "Test\t  Prec@1: 76.830 (Err: 23.170 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [10][0/391]\tLoss 0.5229 (0.5229)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [10][55/391]\tLoss 0.4946 (0.5727)\tPrec@1 78.906 (80.134)\n",
            "Epoch: [10][110/391]\tLoss 0.5894 (0.5655)\tPrec@1 78.906 (80.405)\n",
            "Epoch: [10][165/391]\tLoss 0.4325 (0.5644)\tPrec@1 83.594 (80.624)\n",
            "Epoch: [10][220/391]\tLoss 0.6301 (0.5689)\tPrec@1 78.125 (80.515)\n",
            "Epoch: [10][275/391]\tLoss 0.5792 (0.5688)\tPrec@1 78.906 (80.497)\n",
            "Epoch: [10][330/391]\tLoss 0.4788 (0.5662)\tPrec@1 81.250 (80.558)\n",
            "Epoch: [10][385/391]\tLoss 0.4369 (0.5666)\tPrec@1 84.375 (80.505)\n",
            "Test\t  Prec@1: 77.810 (Err: 22.190 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [11][0/391]\tLoss 0.4802 (0.4802)\tPrec@1 81.250 (81.250)\n",
            "Epoch: [11][55/391]\tLoss 0.4569 (0.5200)\tPrec@1 82.812 (81.571)\n",
            "Epoch: [11][110/391]\tLoss 0.6383 (0.5314)\tPrec@1 77.344 (81.574)\n",
            "Epoch: [11][165/391]\tLoss 0.6609 (0.5330)\tPrec@1 76.562 (81.382)\n",
            "Epoch: [11][220/391]\tLoss 0.4988 (0.5328)\tPrec@1 85.938 (81.434)\n",
            "Epoch: [11][275/391]\tLoss 0.5349 (0.5363)\tPrec@1 78.906 (81.363)\n",
            "Epoch: [11][330/391]\tLoss 0.5582 (0.5383)\tPrec@1 79.688 (81.328)\n",
            "Epoch: [11][385/391]\tLoss 0.4879 (0.5344)\tPrec@1 82.031 (81.428)\n",
            "Test\t  Prec@1: 79.060 (Err: 20.940 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [12][0/391]\tLoss 0.5316 (0.5316)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [12][55/391]\tLoss 0.3687 (0.5026)\tPrec@1 88.281 (82.840)\n",
            "Epoch: [12][110/391]\tLoss 0.4167 (0.5055)\tPrec@1 85.938 (82.693)\n",
            "Epoch: [12][165/391]\tLoss 0.3556 (0.5026)\tPrec@1 86.719 (82.732)\n",
            "Epoch: [12][220/391]\tLoss 0.5406 (0.5063)\tPrec@1 81.250 (82.597)\n",
            "Epoch: [12][275/391]\tLoss 0.5380 (0.5103)\tPrec@1 78.125 (82.374)\n",
            "Epoch: [12][330/391]\tLoss 0.4085 (0.5126)\tPrec@1 83.594 (82.251)\n",
            "Epoch: [12][385/391]\tLoss 0.4178 (0.5110)\tPrec@1 85.156 (82.286)\n",
            "Test\t  Prec@1: 78.760 (Err: 21.240 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [13][0/391]\tLoss 0.5875 (0.5875)\tPrec@1 78.906 (78.906)\n",
            "Epoch: [13][55/391]\tLoss 0.5689 (0.4826)\tPrec@1 80.469 (83.133)\n",
            "Epoch: [13][110/391]\tLoss 0.5093 (0.4904)\tPrec@1 84.375 (82.777)\n",
            "Epoch: [13][165/391]\tLoss 0.5113 (0.4901)\tPrec@1 80.469 (82.893)\n",
            "Epoch: [13][220/391]\tLoss 0.5677 (0.4903)\tPrec@1 82.031 (83.014)\n",
            "Epoch: [13][275/391]\tLoss 0.5143 (0.4914)\tPrec@1 81.250 (82.996)\n",
            "Epoch: [13][330/391]\tLoss 0.4463 (0.4895)\tPrec@1 84.375 (83.063)\n",
            "Epoch: [13][385/391]\tLoss 0.5074 (0.4867)\tPrec@1 82.031 (83.161)\n",
            "Test\t  Prec@1: 80.680 (Err: 19.320 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [14][0/391]\tLoss 0.3895 (0.3895)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [14][55/391]\tLoss 0.4134 (0.4422)\tPrec@1 84.375 (84.724)\n",
            "Epoch: [14][110/391]\tLoss 0.4665 (0.4557)\tPrec@1 85.938 (84.305)\n",
            "Epoch: [14][165/391]\tLoss 0.3657 (0.4571)\tPrec@1 85.156 (84.191)\n",
            "Epoch: [14][220/391]\tLoss 0.4987 (0.4657)\tPrec@1 81.250 (83.763)\n",
            "Epoch: [14][275/391]\tLoss 0.5903 (0.4670)\tPrec@1 78.125 (83.721)\n",
            "Epoch: [14][330/391]\tLoss 0.4274 (0.4663)\tPrec@1 85.938 (83.688)\n",
            "Epoch: [14][385/391]\tLoss 0.6158 (0.4657)\tPrec@1 82.031 (83.746)\n",
            "Test\t  Prec@1: 78.740 (Err: 21.260 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [15][0/391]\tLoss 0.3971 (0.3971)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [15][55/391]\tLoss 0.5741 (0.4438)\tPrec@1 81.250 (84.459)\n",
            "Epoch: [15][110/391]\tLoss 0.4426 (0.4450)\tPrec@1 83.594 (84.445)\n",
            "Epoch: [15][165/391]\tLoss 0.4581 (0.4439)\tPrec@1 86.719 (84.516)\n",
            "Epoch: [15][220/391]\tLoss 0.4920 (0.4481)\tPrec@1 80.469 (84.410)\n",
            "Epoch: [15][275/391]\tLoss 0.4351 (0.4461)\tPrec@1 83.594 (84.381)\n",
            "Epoch: [15][330/391]\tLoss 0.4803 (0.4419)\tPrec@1 85.156 (84.550)\n",
            "Epoch: [15][385/391]\tLoss 0.3894 (0.4451)\tPrec@1 89.844 (84.407)\n",
            "Test\t  Prec@1: 80.910 (Err: 19.090 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [16][0/391]\tLoss 0.5029 (0.5029)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [16][55/391]\tLoss 0.5022 (0.4132)\tPrec@1 84.375 (85.700)\n",
            "Epoch: [16][110/391]\tLoss 0.3156 (0.4124)\tPrec@1 87.500 (85.804)\n",
            "Epoch: [16][165/391]\tLoss 0.4343 (0.4172)\tPrec@1 85.156 (85.585)\n",
            "Epoch: [16][220/391]\tLoss 0.3100 (0.4204)\tPrec@1 89.062 (85.489)\n",
            "Epoch: [16][275/391]\tLoss 0.4227 (0.4210)\tPrec@1 84.375 (85.329)\n",
            "Epoch: [16][330/391]\tLoss 0.3670 (0.4224)\tPrec@1 89.844 (85.317)\n",
            "Epoch: [16][385/391]\tLoss 0.4816 (0.4229)\tPrec@1 80.469 (85.308)\n",
            "Test\t  Prec@1: 81.740 (Err: 18.260 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [17][0/391]\tLoss 0.2435 (0.2435)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [17][55/391]\tLoss 0.3111 (0.4015)\tPrec@1 88.281 (85.924)\n",
            "Epoch: [17][110/391]\tLoss 0.5291 (0.3977)\tPrec@1 80.469 (85.881)\n",
            "Epoch: [17][165/391]\tLoss 0.3921 (0.4004)\tPrec@1 87.500 (86.116)\n",
            "Epoch: [17][220/391]\tLoss 0.5624 (0.4063)\tPrec@1 84.375 (85.909)\n",
            "Epoch: [17][275/391]\tLoss 0.4880 (0.4117)\tPrec@1 82.031 (85.632)\n",
            "Epoch: [17][330/391]\tLoss 0.3694 (0.4104)\tPrec@1 86.719 (85.652)\n",
            "Epoch: [17][385/391]\tLoss 0.2402 (0.4092)\tPrec@1 89.844 (85.701)\n",
            "Test\t  Prec@1: 82.210 (Err: 17.790 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [18][0/391]\tLoss 0.4388 (0.4388)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [18][55/391]\tLoss 0.3237 (0.3868)\tPrec@1 88.281 (86.412)\n",
            "Epoch: [18][110/391]\tLoss 0.5314 (0.3960)\tPrec@1 80.469 (86.205)\n",
            "Epoch: [18][165/391]\tLoss 0.4014 (0.3955)\tPrec@1 84.375 (86.093)\n",
            "Epoch: [18][220/391]\tLoss 0.2396 (0.3978)\tPrec@1 92.969 (86.054)\n",
            "Epoch: [18][275/391]\tLoss 0.5048 (0.3962)\tPrec@1 82.031 (86.099)\n",
            "Epoch: [18][330/391]\tLoss 0.3766 (0.3960)\tPrec@1 85.938 (86.133)\n",
            "Epoch: [18][385/391]\tLoss 0.5744 (0.3966)\tPrec@1 78.906 (86.166)\n",
            "Test\t  Prec@1: 81.130 (Err: 18.870 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [19][0/391]\tLoss 0.4136 (0.4136)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [19][55/391]\tLoss 0.3177 (0.3718)\tPrec@1 86.719 (86.816)\n",
            "Epoch: [19][110/391]\tLoss 0.4715 (0.3689)\tPrec@1 82.812 (86.740)\n",
            "Epoch: [19][165/391]\tLoss 0.3431 (0.3693)\tPrec@1 86.719 (86.766)\n",
            "Epoch: [19][220/391]\tLoss 0.3795 (0.3718)\tPrec@1 86.719 (86.637)\n",
            "Epoch: [19][275/391]\tLoss 0.4619 (0.3713)\tPrec@1 82.812 (86.733)\n",
            "Epoch: [19][330/391]\tLoss 0.4401 (0.3745)\tPrec@1 84.375 (86.631)\n",
            "Epoch: [19][385/391]\tLoss 0.4500 (0.3753)\tPrec@1 85.156 (86.703)\n",
            "Test\t  Prec@1: 84.040 (Err: 15.960 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [20][0/391]\tLoss 0.4262 (0.4262)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [20][55/391]\tLoss 0.2858 (0.3584)\tPrec@1 89.844 (87.765)\n",
            "Epoch: [20][110/391]\tLoss 0.4692 (0.3624)\tPrec@1 87.500 (87.366)\n",
            "Epoch: [20][165/391]\tLoss 0.3908 (0.3546)\tPrec@1 88.281 (87.524)\n",
            "Epoch: [20][220/391]\tLoss 0.4480 (0.3572)\tPrec@1 81.250 (87.316)\n",
            "Epoch: [20][275/391]\tLoss 0.4255 (0.3603)\tPrec@1 85.938 (87.251)\n",
            "Epoch: [20][330/391]\tLoss 0.3262 (0.3615)\tPrec@1 90.625 (87.292)\n",
            "Epoch: [20][385/391]\tLoss 0.4825 (0.3626)\tPrec@1 86.719 (87.267)\n",
            "Test\t  Prec@1: 82.900 (Err: 17.100 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [21][0/391]\tLoss 0.3963 (0.3963)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [21][55/391]\tLoss 0.4090 (0.3313)\tPrec@1 89.062 (88.602)\n",
            "Epoch: [21][110/391]\tLoss 0.3596 (0.3435)\tPrec@1 86.719 (88.077)\n",
            "Epoch: [21][165/391]\tLoss 0.2940 (0.3442)\tPrec@1 91.406 (87.952)\n",
            "Epoch: [21][220/391]\tLoss 0.3141 (0.3462)\tPrec@1 90.625 (87.868)\n",
            "Epoch: [21][275/391]\tLoss 0.2876 (0.3462)\tPrec@1 90.625 (87.868)\n",
            "Epoch: [21][330/391]\tLoss 0.4769 (0.3513)\tPrec@1 82.031 (87.675)\n",
            "Epoch: [21][385/391]\tLoss 0.4515 (0.3482)\tPrec@1 83.594 (87.826)\n",
            "Test\t  Prec@1: 84.050 (Err: 15.950 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [22][0/391]\tLoss 0.3255 (0.3255)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [22][55/391]\tLoss 0.1992 (0.3290)\tPrec@1 92.969 (88.463)\n",
            "Epoch: [22][110/391]\tLoss 0.5244 (0.3269)\tPrec@1 83.594 (88.443)\n",
            "Epoch: [22][165/391]\tLoss 0.2837 (0.3308)\tPrec@1 90.625 (88.559)\n",
            "Epoch: [22][220/391]\tLoss 0.2922 (0.3305)\tPrec@1 92.188 (88.511)\n",
            "Epoch: [22][275/391]\tLoss 0.2435 (0.3333)\tPrec@1 91.406 (88.315)\n",
            "Epoch: [22][330/391]\tLoss 0.3999 (0.3356)\tPrec@1 83.594 (88.225)\n",
            "Epoch: [22][385/391]\tLoss 0.4032 (0.3380)\tPrec@1 87.500 (88.081)\n",
            "Test\t  Prec@1: 82.650 (Err: 17.350 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [23][0/391]\tLoss 0.3204 (0.3204)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [23][55/391]\tLoss 0.4139 (0.3101)\tPrec@1 85.938 (88.923)\n",
            "Epoch: [23][110/391]\tLoss 0.2776 (0.3174)\tPrec@1 89.062 (88.668)\n",
            "Epoch: [23][165/391]\tLoss 0.4031 (0.3292)\tPrec@1 82.812 (88.375)\n",
            "Epoch: [23][220/391]\tLoss 0.2282 (0.3240)\tPrec@1 92.188 (88.741)\n",
            "Epoch: [23][275/391]\tLoss 0.3232 (0.3247)\tPrec@1 87.500 (88.649)\n",
            "Epoch: [23][330/391]\tLoss 0.2394 (0.3263)\tPrec@1 92.188 (88.465)\n",
            "Epoch: [23][385/391]\tLoss 0.3073 (0.3263)\tPrec@1 89.844 (88.437)\n",
            "Test\t  Prec@1: 83.760 (Err: 16.240 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [24][0/391]\tLoss 0.2899 (0.2899)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [24][55/391]\tLoss 0.3492 (0.2969)\tPrec@1 89.844 (89.355)\n",
            "Epoch: [24][110/391]\tLoss 0.4089 (0.3076)\tPrec@1 86.719 (89.062)\n",
            "Epoch: [24][165/391]\tLoss 0.2738 (0.3025)\tPrec@1 90.625 (89.288)\n",
            "Epoch: [24][220/391]\tLoss 0.2458 (0.3083)\tPrec@1 92.188 (89.169)\n",
            "Epoch: [24][275/391]\tLoss 0.4916 (0.3120)\tPrec@1 83.594 (89.088)\n",
            "Epoch: [24][330/391]\tLoss 0.2574 (0.3133)\tPrec@1 88.281 (89.041)\n",
            "Epoch: [24][385/391]\tLoss 0.3237 (0.3168)\tPrec@1 89.062 (88.911)\n",
            "Test\t  Prec@1: 83.370 (Err: 16.630 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [25][0/391]\tLoss 0.2961 (0.2961)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [25][55/391]\tLoss 0.2984 (0.2805)\tPrec@1 89.062 (90.206)\n",
            "Epoch: [25][110/391]\tLoss 0.3357 (0.2844)\tPrec@1 86.719 (90.111)\n",
            "Epoch: [25][165/391]\tLoss 0.2241 (0.2824)\tPrec@1 92.188 (90.197)\n",
            "Epoch: [25][220/391]\tLoss 0.2050 (0.2838)\tPrec@1 92.969 (90.031)\n",
            "Epoch: [25][275/391]\tLoss 0.1686 (0.2927)\tPrec@1 95.312 (89.688)\n",
            "Epoch: [25][330/391]\tLoss 0.3069 (0.2963)\tPrec@1 89.844 (89.520)\n",
            "Epoch: [25][385/391]\tLoss 0.2577 (0.2985)\tPrec@1 90.625 (89.477)\n",
            "Test\t  Prec@1: 83.590 (Err: 16.410 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [26][0/391]\tLoss 0.3072 (0.3072)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [26][55/391]\tLoss 0.3820 (0.2999)\tPrec@1 84.375 (89.342)\n",
            "Epoch: [26][110/391]\tLoss 0.2204 (0.3011)\tPrec@1 93.750 (89.527)\n",
            "Epoch: [26][165/391]\tLoss 0.2853 (0.2923)\tPrec@1 89.062 (89.768)\n",
            "Epoch: [26][220/391]\tLoss 0.1840 (0.2887)\tPrec@1 95.312 (89.847)\n",
            "Epoch: [26][275/391]\tLoss 0.2720 (0.2903)\tPrec@1 90.625 (89.728)\n",
            "Epoch: [26][330/391]\tLoss 0.2331 (0.2935)\tPrec@1 94.531 (89.719)\n",
            "Epoch: [26][385/391]\tLoss 0.3383 (0.2920)\tPrec@1 88.281 (89.720)\n",
            "Test\t  Prec@1: 83.250 (Err: 16.750 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [27][0/391]\tLoss 0.2264 (0.2264)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [27][55/391]\tLoss 0.2313 (0.2561)\tPrec@1 90.625 (90.960)\n",
            "Epoch: [27][110/391]\tLoss 0.3894 (0.2710)\tPrec@1 86.719 (90.372)\n",
            "Epoch: [27][165/391]\tLoss 0.3605 (0.2799)\tPrec@1 86.719 (90.131)\n",
            "Epoch: [27][220/391]\tLoss 0.3489 (0.2800)\tPrec@1 86.719 (90.169)\n",
            "Epoch: [27][275/391]\tLoss 0.2827 (0.2811)\tPrec@1 91.406 (90.101)\n",
            "Epoch: [27][330/391]\tLoss 0.2588 (0.2830)\tPrec@1 91.406 (90.049)\n",
            "Epoch: [27][385/391]\tLoss 0.2419 (0.2822)\tPrec@1 89.844 (90.095)\n",
            "Test\t  Prec@1: 85.440 (Err: 14.560 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [28][0/391]\tLoss 0.1963 (0.1963)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [28][55/391]\tLoss 0.1899 (0.2696)\tPrec@1 95.312 (90.262)\n",
            "Epoch: [28][110/391]\tLoss 0.2433 (0.2710)\tPrec@1 89.844 (90.259)\n",
            "Epoch: [28][165/391]\tLoss 0.2891 (0.2716)\tPrec@1 89.844 (90.216)\n",
            "Epoch: [28][220/391]\tLoss 0.2482 (0.2731)\tPrec@1 92.188 (90.201)\n",
            "Epoch: [28][275/391]\tLoss 0.3457 (0.2728)\tPrec@1 89.062 (90.254)\n",
            "Epoch: [28][330/391]\tLoss 0.2827 (0.2722)\tPrec@1 91.406 (90.273)\n",
            "Epoch: [28][385/391]\tLoss 0.4356 (0.2748)\tPrec@1 85.156 (90.188)\n",
            "Test\t  Prec@1: 84.950 (Err: 15.050 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [29][0/391]\tLoss 0.3387 (0.3387)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [29][55/391]\tLoss 0.2719 (0.2607)\tPrec@1 90.625 (90.904)\n",
            "Epoch: [29][110/391]\tLoss 0.2365 (0.2639)\tPrec@1 91.406 (90.850)\n",
            "Epoch: [29][165/391]\tLoss 0.2185 (0.2645)\tPrec@1 92.969 (90.724)\n",
            "Epoch: [29][220/391]\tLoss 0.2502 (0.2673)\tPrec@1 92.969 (90.632)\n",
            "Epoch: [29][275/391]\tLoss 0.2973 (0.2679)\tPrec@1 89.062 (90.583)\n",
            "Epoch: [29][330/391]\tLoss 0.3220 (0.2663)\tPrec@1 85.938 (90.665)\n",
            "Epoch: [29][385/391]\tLoss 0.2161 (0.2639)\tPrec@1 92.188 (90.771)\n",
            "Test\t  Prec@1: 85.420 (Err: 14.580 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [30][0/391]\tLoss 0.2342 (0.2342)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [30][55/391]\tLoss 0.2358 (0.2412)\tPrec@1 92.188 (91.504)\n",
            "Epoch: [30][110/391]\tLoss 0.2912 (0.2518)\tPrec@1 92.188 (91.125)\n",
            "Epoch: [30][165/391]\tLoss 0.2037 (0.2573)\tPrec@1 92.969 (91.020)\n",
            "Epoch: [30][220/391]\tLoss 0.2863 (0.2552)\tPrec@1 86.719 (91.046)\n",
            "Epoch: [30][275/391]\tLoss 0.2283 (0.2580)\tPrec@1 92.188 (90.866)\n",
            "Epoch: [30][330/391]\tLoss 0.2805 (0.2588)\tPrec@1 93.750 (90.814)\n",
            "Epoch: [30][385/391]\tLoss 0.2765 (0.2602)\tPrec@1 87.500 (90.726)\n",
            "Test\t  Prec@1: 85.260 (Err: 14.740 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [31][0/391]\tLoss 0.1530 (0.1530)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [31][55/391]\tLoss 0.1967 (0.2494)\tPrec@1 91.406 (91.113)\n",
            "Epoch: [31][110/391]\tLoss 0.2540 (0.2430)\tPrec@1 92.188 (91.336)\n",
            "Epoch: [31][165/391]\tLoss 0.1958 (0.2411)\tPrec@1 95.312 (91.402)\n",
            "Epoch: [31][220/391]\tLoss 0.2223 (0.2403)\tPrec@1 91.406 (91.403)\n",
            "Epoch: [31][275/391]\tLoss 0.2578 (0.2439)\tPrec@1 88.281 (91.367)\n",
            "Epoch: [31][330/391]\tLoss 0.2416 (0.2476)\tPrec@1 95.312 (91.210)\n",
            "Epoch: [31][385/391]\tLoss 0.2702 (0.2486)\tPrec@1 89.062 (91.196)\n",
            "Test\t  Prec@1: 85.490 (Err: 14.510 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [32][0/391]\tLoss 0.2009 (0.2009)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [32][55/391]\tLoss 0.2440 (0.2441)\tPrec@1 89.844 (91.281)\n",
            "Epoch: [32][110/391]\tLoss 0.3136 (0.2334)\tPrec@1 89.062 (91.786)\n",
            "Epoch: [32][165/391]\tLoss 0.2731 (0.2321)\tPrec@1 87.500 (91.783)\n",
            "Epoch: [32][220/391]\tLoss 0.2184 (0.2338)\tPrec@1 91.406 (91.746)\n",
            "Epoch: [32][275/391]\tLoss 0.3207 (0.2394)\tPrec@1 89.844 (91.531)\n",
            "Epoch: [32][330/391]\tLoss 0.2647 (0.2390)\tPrec@1 91.406 (91.569)\n",
            "Epoch: [32][385/391]\tLoss 0.2811 (0.2400)\tPrec@1 86.719 (91.518)\n",
            "Test\t  Prec@1: 85.200 (Err: 14.800 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [33][0/391]\tLoss 0.1513 (0.1513)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [33][55/391]\tLoss 0.1641 (0.2208)\tPrec@1 92.969 (92.229)\n",
            "Epoch: [33][110/391]\tLoss 0.3186 (0.2321)\tPrec@1 90.625 (91.878)\n",
            "Epoch: [33][165/391]\tLoss 0.2438 (0.2330)\tPrec@1 89.844 (91.797)\n",
            "Epoch: [33][220/391]\tLoss 0.1919 (0.2316)\tPrec@1 92.188 (91.809)\n",
            "Epoch: [33][275/391]\tLoss 0.3219 (0.2359)\tPrec@1 88.281 (91.602)\n",
            "Epoch: [33][330/391]\tLoss 0.1466 (0.2355)\tPrec@1 95.312 (91.605)\n",
            "Epoch: [33][385/391]\tLoss 0.3348 (0.2383)\tPrec@1 89.844 (91.556)\n",
            "Test\t  Prec@1: 83.710 (Err: 16.290 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [34][0/391]\tLoss 0.1955 (0.1955)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [34][55/391]\tLoss 0.1561 (0.2087)\tPrec@1 94.531 (92.369)\n",
            "Epoch: [34][110/391]\tLoss 0.1987 (0.2146)\tPrec@1 92.188 (92.244)\n",
            "Epoch: [34][165/391]\tLoss 0.1835 (0.2191)\tPrec@1 94.531 (92.140)\n",
            "Epoch: [34][220/391]\tLoss 0.1722 (0.2198)\tPrec@1 94.531 (92.092)\n",
            "Epoch: [34][275/391]\tLoss 0.1882 (0.2197)\tPrec@1 94.531 (92.091)\n",
            "Epoch: [34][330/391]\tLoss 0.2910 (0.2211)\tPrec@1 89.844 (92.081)\n",
            "Epoch: [34][385/391]\tLoss 0.3370 (0.2250)\tPrec@1 85.938 (91.951)\n",
            "Test\t  Prec@1: 86.130 (Err: 13.870 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [35][0/391]\tLoss 0.1280 (0.1280)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [35][55/391]\tLoss 0.2206 (0.2191)\tPrec@1 93.750 (92.243)\n",
            "Epoch: [35][110/391]\tLoss 0.1556 (0.2161)\tPrec@1 93.750 (92.335)\n",
            "Epoch: [35][165/391]\tLoss 0.1583 (0.2110)\tPrec@1 95.312 (92.522)\n",
            "Epoch: [35][220/391]\tLoss 0.1979 (0.2092)\tPrec@1 92.188 (92.640)\n",
            "Epoch: [35][275/391]\tLoss 0.2039 (0.2136)\tPrec@1 93.750 (92.493)\n",
            "Epoch: [35][330/391]\tLoss 0.2611 (0.2155)\tPrec@1 92.188 (92.464)\n",
            "Epoch: [35][385/391]\tLoss 0.2358 (0.2174)\tPrec@1 93.750 (92.380)\n",
            "Test\t  Prec@1: 86.000 (Err: 14.000 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [36][0/391]\tLoss 0.1120 (0.1120)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [36][55/391]\tLoss 0.2164 (0.1959)\tPrec@1 94.531 (93.025)\n",
            "Epoch: [36][110/391]\tLoss 0.2018 (0.1975)\tPrec@1 93.750 (92.983)\n",
            "Epoch: [36][165/391]\tLoss 0.1390 (0.2019)\tPrec@1 94.531 (92.776)\n",
            "Epoch: [36][220/391]\tLoss 0.2190 (0.2039)\tPrec@1 91.406 (92.704)\n",
            "Epoch: [36][275/391]\tLoss 0.1910 (0.2044)\tPrec@1 92.188 (92.669)\n",
            "Epoch: [36][330/391]\tLoss 0.1584 (0.2053)\tPrec@1 94.531 (92.660)\n",
            "Epoch: [36][385/391]\tLoss 0.1891 (0.2087)\tPrec@1 92.969 (92.487)\n",
            "Test\t  Prec@1: 85.520 (Err: 14.480 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [37][0/391]\tLoss 0.2213 (0.2213)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [37][55/391]\tLoss 0.2412 (0.2005)\tPrec@1 91.406 (92.913)\n",
            "Epoch: [37][110/391]\tLoss 0.1677 (0.1899)\tPrec@1 93.750 (93.314)\n",
            "Epoch: [37][165/391]\tLoss 0.1907 (0.1905)\tPrec@1 93.750 (93.261)\n",
            "Epoch: [37][220/391]\tLoss 0.1344 (0.1955)\tPrec@1 96.875 (93.075)\n",
            "Epoch: [37][275/391]\tLoss 0.3404 (0.1982)\tPrec@1 88.281 (93.017)\n",
            "Epoch: [37][330/391]\tLoss 0.1740 (0.1989)\tPrec@1 95.312 (93.028)\n",
            "Epoch: [37][385/391]\tLoss 0.1777 (0.2010)\tPrec@1 94.531 (92.977)\n",
            "Test\t  Prec@1: 78.270 (Err: 21.730 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [38][0/391]\tLoss 0.1028 (0.1028)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [38][55/391]\tLoss 0.3368 (0.2085)\tPrec@1 88.281 (92.927)\n",
            "Epoch: [38][110/391]\tLoss 0.2317 (0.2038)\tPrec@1 92.188 (92.835)\n",
            "Epoch: [38][165/391]\tLoss 0.2048 (0.2088)\tPrec@1 92.969 (92.545)\n",
            "Epoch: [38][220/391]\tLoss 0.1150 (0.2079)\tPrec@1 96.094 (92.537)\n",
            "Epoch: [38][275/391]\tLoss 0.1677 (0.2051)\tPrec@1 94.531 (92.706)\n",
            "Epoch: [38][330/391]\tLoss 0.2758 (0.2059)\tPrec@1 90.625 (92.641)\n",
            "Epoch: [38][385/391]\tLoss 0.2172 (0.2050)\tPrec@1 92.188 (92.722)\n",
            "Test\t  Prec@1: 84.630 (Err: 15.370 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [39][0/391]\tLoss 0.2581 (0.2581)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [39][55/391]\tLoss 0.1727 (0.1980)\tPrec@1 91.406 (92.885)\n",
            "Epoch: [39][110/391]\tLoss 0.1806 (0.1912)\tPrec@1 92.969 (93.180)\n",
            "Epoch: [39][165/391]\tLoss 0.1408 (0.1949)\tPrec@1 96.094 (93.053)\n",
            "Epoch: [39][220/391]\tLoss 0.2197 (0.1980)\tPrec@1 93.750 (92.919)\n",
            "Epoch: [39][275/391]\tLoss 0.1634 (0.1982)\tPrec@1 95.312 (92.969)\n",
            "Epoch: [39][330/391]\tLoss 0.2349 (0.2012)\tPrec@1 94.531 (92.820)\n",
            "Epoch: [39][385/391]\tLoss 0.2640 (0.2011)\tPrec@1 92.969 (92.801)\n",
            "Test\t  Prec@1: 86.840 (Err: 13.160 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [40][0/391]\tLoss 0.1341 (0.1341)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [40][55/391]\tLoss 0.1351 (0.1793)\tPrec@1 95.312 (93.610)\n",
            "Epoch: [40][110/391]\tLoss 0.1378 (0.1742)\tPrec@1 96.094 (93.919)\n",
            "Epoch: [40][165/391]\tLoss 0.1785 (0.1795)\tPrec@1 95.312 (93.750)\n",
            "Epoch: [40][220/391]\tLoss 0.2166 (0.1837)\tPrec@1 92.188 (93.598)\n",
            "Epoch: [40][275/391]\tLoss 0.1810 (0.1869)\tPrec@1 92.969 (93.374)\n",
            "Epoch: [40][330/391]\tLoss 0.1162 (0.1873)\tPrec@1 97.656 (93.391)\n",
            "Epoch: [40][385/391]\tLoss 0.1979 (0.1875)\tPrec@1 92.969 (93.392)\n",
            "Test\t  Prec@1: 87.170 (Err: 12.830 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [41][0/391]\tLoss 0.1513 (0.1513)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [41][55/391]\tLoss 0.1253 (0.1708)\tPrec@1 93.750 (94.043)\n",
            "Epoch: [41][110/391]\tLoss 0.1628 (0.1746)\tPrec@1 93.750 (93.644)\n",
            "Epoch: [41][165/391]\tLoss 0.1685 (0.1753)\tPrec@1 94.531 (93.703)\n",
            "Epoch: [41][220/391]\tLoss 0.1740 (0.1724)\tPrec@1 96.094 (93.881)\n",
            "Epoch: [41][275/391]\tLoss 0.1687 (0.1733)\tPrec@1 94.531 (93.829)\n",
            "Epoch: [41][330/391]\tLoss 0.2119 (0.1754)\tPrec@1 92.188 (93.788)\n",
            "Epoch: [41][385/391]\tLoss 0.2309 (0.1779)\tPrec@1 90.625 (93.705)\n",
            "Test\t  Prec@1: 85.300 (Err: 14.700 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [42][0/391]\tLoss 0.1528 (0.1528)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [42][55/391]\tLoss 0.1009 (0.1589)\tPrec@1 97.656 (94.210)\n",
            "Epoch: [42][110/391]\tLoss 0.1507 (0.1712)\tPrec@1 93.750 (94.032)\n",
            "Epoch: [42][165/391]\tLoss 0.1254 (0.1753)\tPrec@1 93.750 (93.759)\n",
            "Epoch: [42][220/391]\tLoss 0.2597 (0.1761)\tPrec@1 91.406 (93.725)\n",
            "Epoch: [42][275/391]\tLoss 0.2067 (0.1772)\tPrec@1 90.625 (93.679)\n",
            "Epoch: [42][330/391]\tLoss 0.1935 (0.1780)\tPrec@1 91.406 (93.672)\n",
            "Epoch: [42][385/391]\tLoss 0.2454 (0.1792)\tPrec@1 93.750 (93.643)\n",
            "Test\t  Prec@1: 86.470 (Err: 13.530 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [43][0/391]\tLoss 0.1496 (0.1496)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [43][55/391]\tLoss 0.0788 (0.1614)\tPrec@1 96.875 (94.043)\n",
            "Epoch: [43][110/391]\tLoss 0.2090 (0.1613)\tPrec@1 92.969 (94.158)\n",
            "Epoch: [43][165/391]\tLoss 0.0852 (0.1623)\tPrec@1 96.875 (94.127)\n",
            "Epoch: [43][220/391]\tLoss 0.1282 (0.1623)\tPrec@1 96.875 (94.111)\n",
            "Epoch: [43][275/391]\tLoss 0.0755 (0.1632)\tPrec@1 97.656 (94.104)\n",
            "Epoch: [43][330/391]\tLoss 0.1731 (0.1653)\tPrec@1 92.969 (94.064)\n",
            "Epoch: [43][385/391]\tLoss 0.2022 (0.1669)\tPrec@1 95.312 (94.021)\n",
            "Test\t  Prec@1: 84.390 (Err: 15.610 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [44][0/391]\tLoss 0.1263 (0.1263)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [44][55/391]\tLoss 0.1631 (0.1614)\tPrec@1 92.969 (94.294)\n",
            "Epoch: [44][110/391]\tLoss 0.1294 (0.1592)\tPrec@1 94.531 (94.503)\n",
            "Epoch: [44][165/391]\tLoss 0.1352 (0.1570)\tPrec@1 96.875 (94.607)\n",
            "Epoch: [44][220/391]\tLoss 0.1624 (0.1603)\tPrec@1 93.750 (94.436)\n",
            "Epoch: [44][275/391]\tLoss 0.1673 (0.1606)\tPrec@1 96.094 (94.333)\n",
            "Epoch: [44][330/391]\tLoss 0.0730 (0.1633)\tPrec@1 98.438 (94.262)\n",
            "Epoch: [44][385/391]\tLoss 0.1182 (0.1650)\tPrec@1 94.531 (94.167)\n",
            "Test\t  Prec@1: 86.720 (Err: 13.280 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [45][0/391]\tLoss 0.1838 (0.1838)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [45][55/391]\tLoss 0.1344 (0.1525)\tPrec@1 95.312 (94.350)\n",
            "Epoch: [45][110/391]\tLoss 0.1349 (0.1577)\tPrec@1 95.312 (94.236)\n",
            "Epoch: [45][165/391]\tLoss 0.1713 (0.1588)\tPrec@1 94.531 (94.202)\n",
            "Epoch: [45][220/391]\tLoss 0.1686 (0.1632)\tPrec@1 92.969 (94.072)\n",
            "Epoch: [45][275/391]\tLoss 0.2269 (0.1636)\tPrec@1 92.969 (94.087)\n",
            "Epoch: [45][330/391]\tLoss 0.2203 (0.1635)\tPrec@1 89.844 (94.078)\n",
            "Epoch: [45][385/391]\tLoss 0.1550 (0.1646)\tPrec@1 93.750 (94.064)\n",
            "Test\t  Prec@1: 86.140 (Err: 13.860 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [46][0/391]\tLoss 0.0891 (0.0891)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [46][55/391]\tLoss 0.2201 (0.1499)\tPrec@1 91.406 (94.824)\n",
            "Epoch: [46][110/391]\tLoss 0.1785 (0.1577)\tPrec@1 96.094 (94.595)\n",
            "Epoch: [46][165/391]\tLoss 0.1959 (0.1589)\tPrec@1 92.188 (94.541)\n",
            "Epoch: [46][220/391]\tLoss 0.2415 (0.1594)\tPrec@1 92.969 (94.471)\n",
            "Epoch: [46][275/391]\tLoss 0.1517 (0.1599)\tPrec@1 93.750 (94.458)\n",
            "Epoch: [46][330/391]\tLoss 0.1203 (0.1619)\tPrec@1 95.312 (94.347)\n",
            "Epoch: [46][385/391]\tLoss 0.1784 (0.1615)\tPrec@1 92.188 (94.301)\n",
            "Test\t  Prec@1: 87.330 (Err: 12.670 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [47][0/391]\tLoss 0.1509 (0.1509)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [47][55/391]\tLoss 0.1636 (0.1428)\tPrec@1 93.750 (94.866)\n",
            "Epoch: [47][110/391]\tLoss 0.1802 (0.1494)\tPrec@1 95.312 (94.644)\n",
            "Epoch: [47][165/391]\tLoss 0.1958 (0.1522)\tPrec@1 92.969 (94.541)\n",
            "Epoch: [47][220/391]\tLoss 0.2559 (0.1558)\tPrec@1 89.844 (94.439)\n",
            "Epoch: [47][275/391]\tLoss 0.2027 (0.1548)\tPrec@1 92.188 (94.472)\n",
            "Epoch: [47][330/391]\tLoss 0.1184 (0.1551)\tPrec@1 95.312 (94.453)\n",
            "Epoch: [47][385/391]\tLoss 0.1568 (0.1558)\tPrec@1 93.750 (94.438)\n",
            "Test\t  Prec@1: 86.260 (Err: 13.740 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [48][0/391]\tLoss 0.0970 (0.0970)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [48][55/391]\tLoss 0.2182 (0.1322)\tPrec@1 92.188 (95.271)\n",
            "Epoch: [48][110/391]\tLoss 0.1300 (0.1391)\tPrec@1 96.094 (95.073)\n",
            "Epoch: [48][165/391]\tLoss 0.1039 (0.1391)\tPrec@1 96.875 (95.016)\n",
            "Epoch: [48][220/391]\tLoss 0.2548 (0.1410)\tPrec@1 92.969 (94.941)\n",
            "Epoch: [48][275/391]\tLoss 0.1715 (0.1451)\tPrec@1 94.531 (94.902)\n",
            "Epoch: [48][330/391]\tLoss 0.1769 (0.1449)\tPrec@1 95.312 (94.907)\n",
            "Epoch: [48][385/391]\tLoss 0.2373 (0.1482)\tPrec@1 93.750 (94.772)\n",
            "Test\t  Prec@1: 86.400 (Err: 13.600 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [49][0/391]\tLoss 0.2625 (0.2625)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [49][55/391]\tLoss 0.0919 (0.1389)\tPrec@1 96.875 (94.992)\n",
            "Epoch: [49][110/391]\tLoss 0.1500 (0.1386)\tPrec@1 95.312 (94.989)\n",
            "Epoch: [49][165/391]\tLoss 0.1451 (0.1372)\tPrec@1 92.969 (95.101)\n",
            "Epoch: [49][220/391]\tLoss 0.2470 (0.1391)\tPrec@1 92.969 (95.093)\n",
            "Epoch: [49][275/391]\tLoss 0.0946 (0.1404)\tPrec@1 96.875 (95.001)\n",
            "Epoch: [49][330/391]\tLoss 0.1312 (0.1422)\tPrec@1 97.656 (94.951)\n",
            "Epoch: [49][385/391]\tLoss 0.2378 (0.1449)\tPrec@1 90.625 (94.873)\n",
            "Test\t  Prec@1: 86.580 (Err: 13.420 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [50][0/391]\tLoss 0.1025 (0.1025)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [50][55/391]\tLoss 0.1373 (0.1262)\tPrec@1 92.969 (95.536)\n",
            "Epoch: [50][110/391]\tLoss 0.1408 (0.1268)\tPrec@1 95.312 (95.580)\n",
            "Epoch: [50][165/391]\tLoss 0.1847 (0.1346)\tPrec@1 92.969 (95.209)\n",
            "Epoch: [50][220/391]\tLoss 0.1077 (0.1352)\tPrec@1 96.875 (95.221)\n",
            "Epoch: [50][275/391]\tLoss 0.1022 (0.1397)\tPrec@1 96.094 (95.029)\n",
            "Epoch: [50][330/391]\tLoss 0.1520 (0.1418)\tPrec@1 92.188 (94.954)\n",
            "Epoch: [50][385/391]\tLoss 0.1362 (0.1423)\tPrec@1 95.312 (94.948)\n",
            "Test\t  Prec@1: 86.120 (Err: 13.880 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [51][0/391]\tLoss 0.1698 (0.1698)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [51][55/391]\tLoss 0.1160 (0.1288)\tPrec@1 96.875 (95.159)\n",
            "Epoch: [51][110/391]\tLoss 0.1647 (0.1312)\tPrec@1 92.969 (95.144)\n",
            "Epoch: [51][165/391]\tLoss 0.1103 (0.1361)\tPrec@1 96.875 (95.030)\n",
            "Epoch: [51][220/391]\tLoss 0.0836 (0.1364)\tPrec@1 96.875 (95.051)\n",
            "Epoch: [51][275/391]\tLoss 0.2105 (0.1372)\tPrec@1 91.406 (95.063)\n",
            "Epoch: [51][330/391]\tLoss 0.1645 (0.1370)\tPrec@1 96.094 (95.067)\n",
            "Epoch: [51][385/391]\tLoss 0.1132 (0.1407)\tPrec@1 99.219 (94.962)\n",
            "Test\t  Prec@1: 86.200 (Err: 13.800 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [52][0/391]\tLoss 0.2237 (0.2237)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [52][55/391]\tLoss 0.1046 (0.1346)\tPrec@1 94.531 (95.410)\n",
            "Epoch: [52][110/391]\tLoss 0.1557 (0.1253)\tPrec@1 95.312 (95.763)\n",
            "Epoch: [52][165/391]\tLoss 0.1330 (0.1223)\tPrec@1 95.312 (95.887)\n",
            "Epoch: [52][220/391]\tLoss 0.0792 (0.1229)\tPrec@1 96.875 (95.776)\n",
            "Epoch: [52][275/391]\tLoss 0.1259 (0.1272)\tPrec@1 94.531 (95.539)\n",
            "Epoch: [52][330/391]\tLoss 0.1254 (0.1292)\tPrec@1 94.531 (95.452)\n",
            "Epoch: [52][385/391]\tLoss 0.3352 (0.1315)\tPrec@1 92.188 (95.381)\n",
            "Test\t  Prec@1: 86.930 (Err: 13.070 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [53][0/391]\tLoss 0.1236 (0.1236)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [53][55/391]\tLoss 0.0533 (0.1104)\tPrec@1 98.438 (95.982)\n",
            "Epoch: [53][110/391]\tLoss 0.1259 (0.1107)\tPrec@1 96.094 (95.988)\n",
            "Epoch: [53][165/391]\tLoss 0.1079 (0.1196)\tPrec@1 96.094 (95.698)\n",
            "Epoch: [53][220/391]\tLoss 0.1291 (0.1211)\tPrec@1 96.094 (95.726)\n",
            "Epoch: [53][275/391]\tLoss 0.1116 (0.1250)\tPrec@1 93.750 (95.630)\n",
            "Epoch: [53][330/391]\tLoss 0.2117 (0.1305)\tPrec@1 94.531 (95.494)\n",
            "Epoch: [53][385/391]\tLoss 0.0861 (0.1306)\tPrec@1 96.094 (95.501)\n",
            "Test\t  Prec@1: 86.850 (Err: 13.150 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [54][0/391]\tLoss 0.1226 (0.1226)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [54][55/391]\tLoss 0.1352 (0.1192)\tPrec@1 92.969 (95.703)\n",
            "Epoch: [54][110/391]\tLoss 0.0949 (0.1232)\tPrec@1 96.875 (95.580)\n",
            "Epoch: [54][165/391]\tLoss 0.0867 (0.1226)\tPrec@1 96.094 (95.623)\n",
            "Epoch: [54][220/391]\tLoss 0.1297 (0.1242)\tPrec@1 96.094 (95.567)\n",
            "Epoch: [54][275/391]\tLoss 0.0769 (0.1264)\tPrec@1 96.875 (95.488)\n",
            "Epoch: [54][330/391]\tLoss 0.1114 (0.1289)\tPrec@1 96.094 (95.397)\n",
            "Epoch: [54][385/391]\tLoss 0.1283 (0.1291)\tPrec@1 94.531 (95.402)\n",
            "Test\t  Prec@1: 86.810 (Err: 13.190 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [55][0/391]\tLoss 0.1587 (0.1587)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [55][55/391]\tLoss 0.1552 (0.1143)\tPrec@1 92.969 (95.801)\n",
            "Epoch: [55][110/391]\tLoss 0.1195 (0.1187)\tPrec@1 94.531 (95.636)\n",
            "Epoch: [55][165/391]\tLoss 0.0541 (0.1189)\tPrec@1 97.656 (95.647)\n",
            "Epoch: [55][220/391]\tLoss 0.1204 (0.1235)\tPrec@1 96.875 (95.578)\n",
            "Epoch: [55][275/391]\tLoss 0.1152 (0.1213)\tPrec@1 94.531 (95.615)\n",
            "Epoch: [55][330/391]\tLoss 0.1237 (0.1220)\tPrec@1 95.312 (95.598)\n",
            "Epoch: [55][385/391]\tLoss 0.2667 (0.1212)\tPrec@1 89.844 (95.671)\n",
            "Test\t  Prec@1: 86.540 (Err: 13.460 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [56][0/391]\tLoss 0.1670 (0.1670)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [56][55/391]\tLoss 0.1580 (0.1154)\tPrec@1 91.406 (95.857)\n",
            "Epoch: [56][110/391]\tLoss 0.0733 (0.1150)\tPrec@1 97.656 (96.066)\n",
            "Epoch: [56][165/391]\tLoss 0.1159 (0.1113)\tPrec@1 96.094 (96.103)\n",
            "Epoch: [56][220/391]\tLoss 0.1237 (0.1141)\tPrec@1 95.312 (95.935)\n",
            "Epoch: [56][275/391]\tLoss 0.1776 (0.1166)\tPrec@1 94.531 (95.879)\n",
            "Epoch: [56][330/391]\tLoss 0.1792 (0.1180)\tPrec@1 94.531 (95.834)\n",
            "Epoch: [56][385/391]\tLoss 0.1346 (0.1196)\tPrec@1 94.531 (95.744)\n",
            "Test\t  Prec@1: 86.510 (Err: 13.490 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [57][0/391]\tLoss 0.1763 (0.1763)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [57][55/391]\tLoss 0.0835 (0.1211)\tPrec@1 97.656 (95.731)\n",
            "Epoch: [57][110/391]\tLoss 0.1935 (0.1222)\tPrec@1 93.750 (95.601)\n",
            "Epoch: [57][165/391]\tLoss 0.1615 (0.1221)\tPrec@1 93.750 (95.670)\n",
            "Epoch: [57][220/391]\tLoss 0.1498 (0.1207)\tPrec@1 93.750 (95.634)\n",
            "Epoch: [57][275/391]\tLoss 0.1437 (0.1221)\tPrec@1 96.094 (95.533)\n",
            "Epoch: [57][330/391]\tLoss 0.0494 (0.1236)\tPrec@1 97.656 (95.478)\n",
            "Epoch: [57][385/391]\tLoss 0.1382 (0.1231)\tPrec@1 92.969 (95.497)\n",
            "Test\t  Prec@1: 87.690 (Err: 12.310 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [58][0/391]\tLoss 0.0814 (0.0814)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [58][55/391]\tLoss 0.1120 (0.1007)\tPrec@1 96.094 (96.443)\n",
            "Epoch: [58][110/391]\tLoss 0.1163 (0.1021)\tPrec@1 95.312 (96.425)\n",
            "Epoch: [58][165/391]\tLoss 0.0735 (0.1045)\tPrec@1 96.875 (96.320)\n",
            "Epoch: [58][220/391]\tLoss 0.1592 (0.1105)\tPrec@1 94.531 (96.104)\n",
            "Epoch: [58][275/391]\tLoss 0.1147 (0.1124)\tPrec@1 95.312 (96.051)\n",
            "Epoch: [58][330/391]\tLoss 0.1335 (0.1136)\tPrec@1 96.875 (95.940)\n",
            "Epoch: [58][385/391]\tLoss 0.1053 (0.1158)\tPrec@1 94.531 (95.865)\n",
            "Test\t  Prec@1: 86.270 (Err: 13.730 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [59][0/391]\tLoss 0.1287 (0.1287)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [59][55/391]\tLoss 0.1574 (0.1183)\tPrec@1 95.312 (95.731)\n",
            "Epoch: [59][110/391]\tLoss 0.1726 (0.1118)\tPrec@1 92.188 (95.988)\n",
            "Epoch: [59][165/391]\tLoss 0.1992 (0.1088)\tPrec@1 94.531 (96.131)\n",
            "Epoch: [59][220/391]\tLoss 0.1375 (0.1049)\tPrec@1 96.094 (96.288)\n",
            "Epoch: [59][275/391]\tLoss 0.0824 (0.1036)\tPrec@1 97.656 (96.382)\n",
            "Epoch: [59][330/391]\tLoss 0.1454 (0.1061)\tPrec@1 94.531 (96.264)\n",
            "Epoch: [59][385/391]\tLoss 0.0439 (0.1074)\tPrec@1 100.000 (96.239)\n",
            "Test\t  Prec@1: 86.880 (Err: 13.120 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [60][0/391]\tLoss 0.0847 (0.0847)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [60][55/391]\tLoss 0.0801 (0.1104)\tPrec@1 96.094 (95.857)\n",
            "Epoch: [60][110/391]\tLoss 0.1884 (0.1118)\tPrec@1 92.969 (95.869)\n",
            "Epoch: [60][165/391]\tLoss 0.1423 (0.1079)\tPrec@1 95.312 (96.066)\n",
            "Epoch: [60][220/391]\tLoss 0.1392 (0.1085)\tPrec@1 96.875 (96.062)\n",
            "Epoch: [60][275/391]\tLoss 0.0856 (0.1087)\tPrec@1 96.875 (96.097)\n",
            "Epoch: [60][330/391]\tLoss 0.2632 (0.1095)\tPrec@1 90.625 (96.025)\n",
            "Epoch: [60][385/391]\tLoss 0.0775 (0.1105)\tPrec@1 96.094 (95.982)\n",
            "Test\t  Prec@1: 87.400 (Err: 12.600 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [61][0/391]\tLoss 0.1618 (0.1618)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [61][55/391]\tLoss 0.0617 (0.1093)\tPrec@1 96.875 (96.177)\n",
            "Epoch: [61][110/391]\tLoss 0.1078 (0.1068)\tPrec@1 96.875 (96.213)\n",
            "Epoch: [61][165/391]\tLoss 0.1588 (0.1091)\tPrec@1 92.969 (96.080)\n",
            "Epoch: [61][220/391]\tLoss 0.1585 (0.1083)\tPrec@1 94.531 (96.115)\n",
            "Epoch: [61][275/391]\tLoss 0.0512 (0.1077)\tPrec@1 98.438 (96.173)\n",
            "Epoch: [61][330/391]\tLoss 0.0875 (0.1088)\tPrec@1 97.656 (96.108)\n",
            "Epoch: [61][385/391]\tLoss 0.0697 (0.1099)\tPrec@1 98.438 (96.080)\n",
            "Test\t  Prec@1: 86.530 (Err: 13.470 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [62][0/391]\tLoss 0.1343 (0.1343)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [62][55/391]\tLoss 0.0897 (0.1018)\tPrec@1 97.656 (96.401)\n",
            "Epoch: [62][110/391]\tLoss 0.1376 (0.0970)\tPrec@1 94.531 (96.544)\n",
            "Epoch: [62][165/391]\tLoss 0.0869 (0.0979)\tPrec@1 96.094 (96.508)\n",
            "Epoch: [62][220/391]\tLoss 0.0995 (0.0989)\tPrec@1 96.875 (96.408)\n",
            "Epoch: [62][275/391]\tLoss 0.2002 (0.1039)\tPrec@1 93.750 (96.241)\n",
            "Epoch: [62][330/391]\tLoss 0.1620 (0.1047)\tPrec@1 95.312 (96.212)\n",
            "Epoch: [62][385/391]\tLoss 0.0751 (0.1057)\tPrec@1 96.875 (96.175)\n",
            "Test\t  Prec@1: 87.170 (Err: 12.830 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [63][0/391]\tLoss 0.0841 (0.0841)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [63][55/391]\tLoss 0.1223 (0.0888)\tPrec@1 94.531 (96.791)\n",
            "Epoch: [63][110/391]\tLoss 0.1070 (0.0901)\tPrec@1 94.531 (96.791)\n",
            "Epoch: [63][165/391]\tLoss 0.1373 (0.0947)\tPrec@1 96.094 (96.621)\n",
            "Epoch: [63][220/391]\tLoss 0.1093 (0.0967)\tPrec@1 95.312 (96.557)\n",
            "Epoch: [63][275/391]\tLoss 0.0857 (0.0976)\tPrec@1 97.656 (96.513)\n",
            "Epoch: [63][330/391]\tLoss 0.0936 (0.0998)\tPrec@1 97.656 (96.467)\n",
            "Epoch: [63][385/391]\tLoss 0.0794 (0.1014)\tPrec@1 96.875 (96.403)\n",
            "Test\t  Prec@1: 87.180 (Err: 12.820 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [64][0/391]\tLoss 0.0436 (0.0436)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [64][55/391]\tLoss 0.0708 (0.0940)\tPrec@1 95.312 (96.652)\n",
            "Epoch: [64][110/391]\tLoss 0.2120 (0.0949)\tPrec@1 94.531 (96.643)\n",
            "Epoch: [64][165/391]\tLoss 0.0411 (0.0949)\tPrec@1 99.219 (96.673)\n",
            "Epoch: [64][220/391]\tLoss 0.0857 (0.0942)\tPrec@1 97.656 (96.638)\n",
            "Epoch: [64][275/391]\tLoss 0.0518 (0.0947)\tPrec@1 98.438 (96.612)\n",
            "Epoch: [64][330/391]\tLoss 0.1501 (0.0948)\tPrec@1 92.969 (96.596)\n",
            "Epoch: [64][385/391]\tLoss 0.1241 (0.0960)\tPrec@1 96.094 (96.545)\n",
            "Test\t  Prec@1: 87.380 (Err: 12.620 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [65][0/391]\tLoss 0.1056 (0.1056)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [65][55/391]\tLoss 0.1078 (0.0929)\tPrec@1 93.750 (96.596)\n",
            "Epoch: [65][110/391]\tLoss 0.0628 (0.0897)\tPrec@1 97.656 (96.720)\n",
            "Epoch: [65][165/391]\tLoss 0.0822 (0.0886)\tPrec@1 97.656 (96.786)\n",
            "Epoch: [65][220/391]\tLoss 0.1152 (0.0900)\tPrec@1 96.094 (96.765)\n",
            "Epoch: [65][275/391]\tLoss 0.0810 (0.0913)\tPrec@1 98.438 (96.739)\n",
            "Epoch: [65][330/391]\tLoss 0.0697 (0.0912)\tPrec@1 97.656 (96.769)\n",
            "Epoch: [65][385/391]\tLoss 0.1523 (0.0923)\tPrec@1 93.750 (96.743)\n",
            "Test\t  Prec@1: 86.780 (Err: 13.220 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [66][0/391]\tLoss 0.0561 (0.0561)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [66][55/391]\tLoss 0.0259 (0.0780)\tPrec@1 100.000 (97.307)\n",
            "Epoch: [66][110/391]\tLoss 0.0344 (0.0771)\tPrec@1 100.000 (97.318)\n",
            "Epoch: [66][165/391]\tLoss 0.1274 (0.0811)\tPrec@1 94.531 (97.204)\n",
            "Epoch: [66][220/391]\tLoss 0.0999 (0.0841)\tPrec@1 96.094 (97.073)\n",
            "Epoch: [66][275/391]\tLoss 0.1637 (0.0844)\tPrec@1 95.312 (97.019)\n",
            "Epoch: [66][330/391]\tLoss 0.0345 (0.0858)\tPrec@1 100.000 (96.955)\n",
            "Epoch: [66][385/391]\tLoss 0.0811 (0.0858)\tPrec@1 96.094 (96.934)\n",
            "Test\t  Prec@1: 87.310 (Err: 12.690 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [67][0/391]\tLoss 0.1189 (0.1189)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [67][55/391]\tLoss 0.0734 (0.1016)\tPrec@1 96.094 (96.359)\n",
            "Epoch: [67][110/391]\tLoss 0.0436 (0.0933)\tPrec@1 97.656 (96.558)\n",
            "Epoch: [67][165/391]\tLoss 0.1770 (0.0961)\tPrec@1 95.312 (96.517)\n",
            "Epoch: [67][220/391]\tLoss 0.0861 (0.0966)\tPrec@1 97.656 (96.546)\n",
            "Epoch: [67][275/391]\tLoss 0.1196 (0.0976)\tPrec@1 95.312 (96.482)\n",
            "Epoch: [67][330/391]\tLoss 0.1114 (0.0983)\tPrec@1 96.094 (96.443)\n",
            "Epoch: [67][385/391]\tLoss 0.0707 (0.0984)\tPrec@1 98.438 (96.456)\n",
            "Test\t  Prec@1: 86.450 (Err: 13.550 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [68][0/391]\tLoss 0.1015 (0.1015)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [68][55/391]\tLoss 0.1523 (0.0907)\tPrec@1 93.750 (96.805)\n",
            "Epoch: [68][110/391]\tLoss 0.1063 (0.0875)\tPrec@1 97.656 (96.917)\n",
            "Epoch: [68][165/391]\tLoss 0.0771 (0.0895)\tPrec@1 97.656 (96.866)\n",
            "Epoch: [68][220/391]\tLoss 0.0859 (0.0912)\tPrec@1 96.094 (96.811)\n",
            "Epoch: [68][275/391]\tLoss 0.0338 (0.0925)\tPrec@1 98.438 (96.736)\n",
            "Epoch: [68][330/391]\tLoss 0.1455 (0.0926)\tPrec@1 96.875 (96.696)\n",
            "Epoch: [68][385/391]\tLoss 0.0621 (0.0923)\tPrec@1 98.438 (96.685)\n",
            "Test\t  Prec@1: 87.250 (Err: 12.750 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [69][0/391]\tLoss 0.1372 (0.1372)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [69][55/391]\tLoss 0.0975 (0.0864)\tPrec@1 95.312 (96.959)\n",
            "Epoch: [69][110/391]\tLoss 0.0777 (0.0819)\tPrec@1 96.875 (97.297)\n",
            "Epoch: [69][165/391]\tLoss 0.0719 (0.0811)\tPrec@1 97.656 (97.275)\n",
            "Epoch: [69][220/391]\tLoss 0.0892 (0.0837)\tPrec@1 96.094 (97.112)\n",
            "Epoch: [69][275/391]\tLoss 0.0687 (0.0870)\tPrec@1 97.656 (97.005)\n",
            "Epoch: [69][330/391]\tLoss 0.0983 (0.0890)\tPrec@1 96.094 (96.887)\n",
            "Epoch: [69][385/391]\tLoss 0.0595 (0.0907)\tPrec@1 97.656 (96.816)\n",
            "Test\t  Prec@1: 86.560 (Err: 13.440 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [70][0/391]\tLoss 0.0713 (0.0713)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [70][55/391]\tLoss 0.0502 (0.0757)\tPrec@1 99.219 (97.503)\n",
            "Epoch: [70][110/391]\tLoss 0.0655 (0.0720)\tPrec@1 96.875 (97.480)\n",
            "Epoch: [70][165/391]\tLoss 0.0889 (0.0733)\tPrec@1 95.312 (97.416)\n",
            "Epoch: [70][220/391]\tLoss 0.0669 (0.0797)\tPrec@1 99.219 (97.267)\n",
            "Epoch: [70][275/391]\tLoss 0.1150 (0.0811)\tPrec@1 95.312 (97.198)\n",
            "Epoch: [70][330/391]\tLoss 0.1247 (0.0831)\tPrec@1 94.531 (97.111)\n",
            "Epoch: [70][385/391]\tLoss 0.1010 (0.0852)\tPrec@1 94.531 (97.005)\n",
            "Test\t  Prec@1: 85.350 (Err: 14.650 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [71][0/391]\tLoss 0.0659 (0.0659)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [71][55/391]\tLoss 0.0638 (0.0746)\tPrec@1 97.656 (97.573)\n",
            "Epoch: [71][110/391]\tLoss 0.1212 (0.0759)\tPrec@1 93.750 (97.361)\n",
            "Epoch: [71][165/391]\tLoss 0.0536 (0.0779)\tPrec@1 97.656 (97.313)\n",
            "Epoch: [71][220/391]\tLoss 0.1086 (0.0801)\tPrec@1 96.875 (97.165)\n",
            "Epoch: [71][275/391]\tLoss 0.0826 (0.0816)\tPrec@1 95.312 (97.130)\n",
            "Epoch: [71][330/391]\tLoss 0.0744 (0.0828)\tPrec@1 96.875 (97.092)\n",
            "Epoch: [71][385/391]\tLoss 0.1342 (0.0827)\tPrec@1 95.312 (97.096)\n",
            "Test\t  Prec@1: 87.040 (Err: 12.960 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [72][0/391]\tLoss 0.1130 (0.1130)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [72][55/391]\tLoss 0.1082 (0.0809)\tPrec@1 97.656 (97.196)\n",
            "Epoch: [72][110/391]\tLoss 0.0848 (0.0843)\tPrec@1 96.875 (97.093)\n",
            "Epoch: [72][165/391]\tLoss 0.1260 (0.0859)\tPrec@1 96.094 (97.007)\n",
            "Epoch: [72][220/391]\tLoss 0.0896 (0.0855)\tPrec@1 97.656 (97.027)\n",
            "Epoch: [72][275/391]\tLoss 0.0791 (0.0869)\tPrec@1 96.875 (96.971)\n",
            "Epoch: [72][330/391]\tLoss 0.0993 (0.0887)\tPrec@1 96.094 (96.889)\n",
            "Epoch: [72][385/391]\tLoss 0.0417 (0.0873)\tPrec@1 98.438 (96.922)\n",
            "Test\t  Prec@1: 87.800 (Err: 12.200 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [73][0/391]\tLoss 0.0747 (0.0747)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [73][55/391]\tLoss 0.0378 (0.0656)\tPrec@1 98.438 (97.656)\n",
            "Epoch: [73][110/391]\tLoss 0.0629 (0.0731)\tPrec@1 98.438 (97.347)\n",
            "Epoch: [73][165/391]\tLoss 0.0495 (0.0763)\tPrec@1 97.656 (97.284)\n",
            "Epoch: [73][220/391]\tLoss 0.0767 (0.0791)\tPrec@1 97.656 (97.161)\n",
            "Epoch: [73][275/391]\tLoss 0.1242 (0.0808)\tPrec@1 97.656 (97.133)\n",
            "Epoch: [73][330/391]\tLoss 0.0633 (0.0815)\tPrec@1 97.656 (97.120)\n",
            "Epoch: [73][385/391]\tLoss 0.0641 (0.0816)\tPrec@1 98.438 (97.120)\n",
            "Test\t  Prec@1: 88.450 (Err: 11.550 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [74][0/391]\tLoss 0.1086 (0.1086)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [74][55/391]\tLoss 0.0506 (0.0734)\tPrec@1 98.438 (97.461)\n",
            "Epoch: [74][110/391]\tLoss 0.0469 (0.0687)\tPrec@1 97.656 (97.565)\n",
            "Epoch: [74][165/391]\tLoss 0.0768 (0.0724)\tPrec@1 96.094 (97.440)\n",
            "Epoch: [74][220/391]\tLoss 0.0292 (0.0735)\tPrec@1 98.438 (97.384)\n",
            "Epoch: [74][275/391]\tLoss 0.0759 (0.0736)\tPrec@1 96.094 (97.387)\n",
            "Epoch: [74][330/391]\tLoss 0.0797 (0.0739)\tPrec@1 97.656 (97.371)\n",
            "Epoch: [74][385/391]\tLoss 0.1467 (0.0742)\tPrec@1 96.875 (97.373)\n",
            "Test\t  Prec@1: 85.380 (Err: 14.620 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [75][0/391]\tLoss 0.1657 (0.1657)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [75][55/391]\tLoss 0.1391 (0.0733)\tPrec@1 96.875 (97.586)\n",
            "Epoch: [75][110/391]\tLoss 0.0901 (0.0739)\tPrec@1 96.875 (97.501)\n",
            "Epoch: [75][165/391]\tLoss 0.1173 (0.0754)\tPrec@1 96.094 (97.412)\n",
            "Epoch: [75][220/391]\tLoss 0.0450 (0.0754)\tPrec@1 98.438 (97.342)\n",
            "Epoch: [75][275/391]\tLoss 0.0342 (0.0739)\tPrec@1 99.219 (97.413)\n",
            "Epoch: [75][330/391]\tLoss 0.0998 (0.0756)\tPrec@1 98.438 (97.356)\n",
            "Epoch: [75][385/391]\tLoss 0.1134 (0.0758)\tPrec@1 95.312 (97.322)\n",
            "Test\t  Prec@1: 87.520 (Err: 12.480 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [76][0/391]\tLoss 0.0723 (0.0723)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [76][55/391]\tLoss 0.0650 (0.0686)\tPrec@1 97.656 (97.419)\n",
            "Epoch: [76][110/391]\tLoss 0.0656 (0.0663)\tPrec@1 99.219 (97.600)\n",
            "Epoch: [76][165/391]\tLoss 0.1020 (0.0686)\tPrec@1 96.875 (97.449)\n",
            "Epoch: [76][220/391]\tLoss 0.0544 (0.0696)\tPrec@1 98.438 (97.479)\n",
            "Epoch: [76][275/391]\tLoss 0.1119 (0.0715)\tPrec@1 96.094 (97.433)\n",
            "Epoch: [76][330/391]\tLoss 0.1287 (0.0724)\tPrec@1 96.094 (97.382)\n",
            "Epoch: [76][385/391]\tLoss 0.0370 (0.0737)\tPrec@1 98.438 (97.371)\n",
            "Test\t  Prec@1: 86.760 (Err: 13.240 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [77][0/391]\tLoss 0.0573 (0.0573)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [77][55/391]\tLoss 0.0916 (0.0678)\tPrec@1 97.656 (97.698)\n",
            "Epoch: [77][110/391]\tLoss 0.0368 (0.0680)\tPrec@1 98.438 (97.656)\n",
            "Epoch: [77][165/391]\tLoss 0.1271 (0.0703)\tPrec@1 96.094 (97.590)\n",
            "Epoch: [77][220/391]\tLoss 0.1358 (0.0705)\tPrec@1 95.312 (97.607)\n",
            "Epoch: [77][275/391]\tLoss 0.0313 (0.0715)\tPrec@1 98.438 (97.563)\n",
            "Epoch: [77][330/391]\tLoss 0.1382 (0.0717)\tPrec@1 95.312 (97.541)\n",
            "Epoch: [77][385/391]\tLoss 0.1234 (0.0734)\tPrec@1 95.312 (97.454)\n",
            "Test\t  Prec@1: 87.170 (Err: 12.830 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [78][0/391]\tLoss 0.0787 (0.0787)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [78][55/391]\tLoss 0.0319 (0.0770)\tPrec@1 98.438 (97.210)\n",
            "Epoch: [78][110/391]\tLoss 0.1219 (0.0743)\tPrec@1 96.094 (97.332)\n",
            "Epoch: [78][165/391]\tLoss 0.1180 (0.0739)\tPrec@1 95.312 (97.336)\n",
            "Epoch: [78][220/391]\tLoss 0.0464 (0.0727)\tPrec@1 96.875 (97.377)\n",
            "Epoch: [78][275/391]\tLoss 0.0836 (0.0732)\tPrec@1 96.875 (97.325)\n",
            "Epoch: [78][330/391]\tLoss 0.0726 (0.0723)\tPrec@1 97.656 (97.380)\n",
            "Epoch: [78][385/391]\tLoss 0.1403 (0.0732)\tPrec@1 95.312 (97.363)\n",
            "Test\t  Prec@1: 87.740 (Err: 12.260 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [79][0/391]\tLoss 0.0519 (0.0519)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [79][55/391]\tLoss 0.0425 (0.0710)\tPrec@1 99.219 (97.559)\n",
            "Epoch: [79][110/391]\tLoss 0.0648 (0.0649)\tPrec@1 98.438 (97.790)\n",
            "Epoch: [79][165/391]\tLoss 0.0900 (0.0649)\tPrec@1 96.094 (97.760)\n",
            "Epoch: [79][220/391]\tLoss 0.0645 (0.0680)\tPrec@1 97.656 (97.653)\n",
            "Epoch: [79][275/391]\tLoss 0.1269 (0.0706)\tPrec@1 96.875 (97.549)\n",
            "Epoch: [79][330/391]\tLoss 0.0601 (0.0723)\tPrec@1 98.438 (97.453)\n",
            "Epoch: [79][385/391]\tLoss 0.0885 (0.0723)\tPrec@1 97.656 (97.466)\n",
            "Test\t  Prec@1: 87.360 (Err: 12.640 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [80][0/391]\tLoss 0.0348 (0.0348)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [80][55/391]\tLoss 0.0483 (0.0729)\tPrec@1 98.438 (97.419)\n",
            "Epoch: [80][110/391]\tLoss 0.0830 (0.0716)\tPrec@1 96.875 (97.438)\n",
            "Epoch: [80][165/391]\tLoss 0.1136 (0.0727)\tPrec@1 96.094 (97.412)\n",
            "Epoch: [80][220/391]\tLoss 0.0713 (0.0728)\tPrec@1 96.875 (97.419)\n",
            "Epoch: [80][275/391]\tLoss 0.0859 (0.0731)\tPrec@1 96.094 (97.399)\n",
            "Epoch: [80][330/391]\tLoss 0.0855 (0.0729)\tPrec@1 96.094 (97.397)\n",
            "Epoch: [80][385/391]\tLoss 0.0575 (0.0729)\tPrec@1 97.656 (97.395)\n",
            "Test\t  Prec@1: 87.990 (Err: 12.010 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [81][0/391]\tLoss 0.0609 (0.0609)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [81][55/391]\tLoss 0.0425 (0.0600)\tPrec@1 97.656 (98.103)\n",
            "Epoch: [81][110/391]\tLoss 0.0840 (0.0616)\tPrec@1 96.094 (97.966)\n",
            "Epoch: [81][165/391]\tLoss 0.0451 (0.0624)\tPrec@1 98.438 (97.892)\n",
            "Epoch: [81][220/391]\tLoss 0.0830 (0.0640)\tPrec@1 96.094 (97.780)\n",
            "Epoch: [81][275/391]\tLoss 0.0977 (0.0654)\tPrec@1 94.531 (97.738)\n",
            "Epoch: [81][330/391]\tLoss 0.0247 (0.0646)\tPrec@1 99.219 (97.770)\n",
            "Epoch: [81][385/391]\tLoss 0.0637 (0.0653)\tPrec@1 97.656 (97.741)\n",
            "Test\t  Prec@1: 87.860 (Err: 12.140 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [82][0/391]\tLoss 0.0873 (0.0873)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [82][55/391]\tLoss 0.0367 (0.0623)\tPrec@1 98.438 (97.810)\n",
            "Epoch: [82][110/391]\tLoss 0.0445 (0.0590)\tPrec@1 97.656 (97.973)\n",
            "Epoch: [82][165/391]\tLoss 0.0283 (0.0564)\tPrec@1 99.219 (98.061)\n",
            "Epoch: [82][220/391]\tLoss 0.0549 (0.0611)\tPrec@1 97.656 (97.918)\n",
            "Epoch: [82][275/391]\tLoss 0.1699 (0.0646)\tPrec@1 96.094 (97.826)\n",
            "Epoch: [82][330/391]\tLoss 0.1035 (0.0658)\tPrec@1 98.438 (97.760)\n",
            "Epoch: [82][385/391]\tLoss 0.1559 (0.0676)\tPrec@1 95.312 (97.666)\n",
            "Test\t  Prec@1: 86.650 (Err: 13.350 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [83][0/391]\tLoss 0.0517 (0.0517)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [83][55/391]\tLoss 0.1178 (0.0712)\tPrec@1 96.875 (97.307)\n",
            "Epoch: [83][110/391]\tLoss 0.1587 (0.0710)\tPrec@1 92.969 (97.368)\n",
            "Epoch: [83][165/391]\tLoss 0.0530 (0.0693)\tPrec@1 97.656 (97.487)\n",
            "Epoch: [83][220/391]\tLoss 0.0502 (0.0686)\tPrec@1 97.656 (97.497)\n",
            "Epoch: [83][275/391]\tLoss 0.0723 (0.0703)\tPrec@1 96.094 (97.438)\n",
            "Epoch: [83][330/391]\tLoss 0.0561 (0.0709)\tPrec@1 98.438 (97.439)\n",
            "Epoch: [83][385/391]\tLoss 0.0725 (0.0712)\tPrec@1 96.875 (97.452)\n",
            "Test\t  Prec@1: 87.800 (Err: 12.200 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [84][0/391]\tLoss 0.0371 (0.0371)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [84][55/391]\tLoss 0.0445 (0.0540)\tPrec@1 98.438 (98.242)\n",
            "Epoch: [84][110/391]\tLoss 0.0509 (0.0539)\tPrec@1 98.438 (98.128)\n",
            "Epoch: [84][165/391]\tLoss 0.0185 (0.0533)\tPrec@1 100.000 (98.113)\n",
            "Epoch: [84][220/391]\tLoss 0.1199 (0.0568)\tPrec@1 95.312 (97.989)\n",
            "Epoch: [84][275/391]\tLoss 0.0523 (0.0573)\tPrec@1 99.219 (97.993)\n",
            "Epoch: [84][330/391]\tLoss 0.0429 (0.0602)\tPrec@1 98.438 (97.906)\n",
            "Epoch: [84][385/391]\tLoss 0.1258 (0.0624)\tPrec@1 96.094 (97.812)\n",
            "Test\t  Prec@1: 88.170 (Err: 11.830 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [85][0/391]\tLoss 0.0446 (0.0446)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [85][55/391]\tLoss 0.0269 (0.0640)\tPrec@1 99.219 (97.782)\n",
            "Epoch: [85][110/391]\tLoss 0.0789 (0.0640)\tPrec@1 96.875 (97.706)\n",
            "Epoch: [85][165/391]\tLoss 0.0784 (0.0651)\tPrec@1 96.875 (97.670)\n",
            "Epoch: [85][220/391]\tLoss 0.1224 (0.0666)\tPrec@1 95.312 (97.663)\n",
            "Epoch: [85][275/391]\tLoss 0.0466 (0.0676)\tPrec@1 98.438 (97.631)\n",
            "Epoch: [85][330/391]\tLoss 0.0176 (0.0658)\tPrec@1 100.000 (97.687)\n",
            "Epoch: [85][385/391]\tLoss 0.1337 (0.0654)\tPrec@1 97.656 (97.719)\n",
            "Test\t  Prec@1: 87.630 (Err: 12.370 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [86][0/391]\tLoss 0.0509 (0.0509)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [86][55/391]\tLoss 0.0312 (0.0540)\tPrec@1 99.219 (98.145)\n",
            "Epoch: [86][110/391]\tLoss 0.0743 (0.0574)\tPrec@1 97.656 (98.036)\n",
            "Epoch: [86][165/391]\tLoss 0.0301 (0.0592)\tPrec@1 99.219 (97.948)\n",
            "Epoch: [86][220/391]\tLoss 0.0677 (0.0607)\tPrec@1 97.656 (97.914)\n",
            "Epoch: [86][275/391]\tLoss 0.1235 (0.0608)\tPrec@1 96.094 (97.917)\n",
            "Epoch: [86][330/391]\tLoss 0.0440 (0.0601)\tPrec@1 97.656 (97.932)\n",
            "Epoch: [86][385/391]\tLoss 0.0253 (0.0606)\tPrec@1 100.000 (97.899)\n",
            "Test\t  Prec@1: 87.950 (Err: 12.050 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [87][0/391]\tLoss 0.0438 (0.0438)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [87][55/391]\tLoss 0.0462 (0.0519)\tPrec@1 98.438 (98.214)\n",
            "Epoch: [87][110/391]\tLoss 0.0648 (0.0547)\tPrec@1 97.656 (98.142)\n",
            "Epoch: [87][165/391]\tLoss 0.0285 (0.0520)\tPrec@1 99.219 (98.155)\n",
            "Epoch: [87][220/391]\tLoss 0.0365 (0.0534)\tPrec@1 98.438 (98.098)\n",
            "Epoch: [87][275/391]\tLoss 0.0368 (0.0544)\tPrec@1 99.219 (98.072)\n",
            "Epoch: [87][330/391]\tLoss 0.0362 (0.0552)\tPrec@1 98.438 (98.048)\n",
            "Epoch: [87][385/391]\tLoss 0.0563 (0.0550)\tPrec@1 97.656 (98.065)\n",
            "Test\t  Prec@1: 88.180 (Err: 11.820 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [88][0/391]\tLoss 0.0521 (0.0521)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [88][55/391]\tLoss 0.0802 (0.0535)\tPrec@1 96.875 (98.172)\n",
            "Epoch: [88][110/391]\tLoss 0.0252 (0.0567)\tPrec@1 100.000 (98.050)\n",
            "Epoch: [88][165/391]\tLoss 0.0378 (0.0564)\tPrec@1 99.219 (98.028)\n",
            "Epoch: [88][220/391]\tLoss 0.0922 (0.0550)\tPrec@1 96.875 (98.038)\n",
            "Epoch: [88][275/391]\tLoss 0.0942 (0.0566)\tPrec@1 97.656 (98.027)\n",
            "Epoch: [88][330/391]\tLoss 0.0467 (0.0569)\tPrec@1 97.656 (98.015)\n",
            "Epoch: [88][385/391]\tLoss 0.0940 (0.0591)\tPrec@1 96.094 (97.936)\n",
            "Test\t  Prec@1: 87.180 (Err: 12.820 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [89][0/391]\tLoss 0.1182 (0.1182)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [89][55/391]\tLoss 0.0438 (0.0691)\tPrec@1 99.219 (97.768)\n",
            "Epoch: [89][110/391]\tLoss 0.0507 (0.0657)\tPrec@1 98.438 (97.832)\n",
            "Epoch: [89][165/391]\tLoss 0.0159 (0.0646)\tPrec@1 100.000 (97.826)\n",
            "Epoch: [89][220/391]\tLoss 0.0431 (0.0650)\tPrec@1 99.219 (97.769)\n",
            "Epoch: [89][275/391]\tLoss 0.0584 (0.0648)\tPrec@1 97.656 (97.792)\n",
            "Epoch: [89][330/391]\tLoss 0.0521 (0.0684)\tPrec@1 97.656 (97.668)\n",
            "Epoch: [89][385/391]\tLoss 0.1126 (0.0701)\tPrec@1 96.875 (97.606)\n",
            "Test\t  Prec@1: 86.400 (Err: 13.600 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [90][0/391]\tLoss 0.0361 (0.0361)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [90][55/391]\tLoss 0.0519 (0.0629)\tPrec@1 98.438 (97.782)\n",
            "Epoch: [90][110/391]\tLoss 0.0241 (0.0593)\tPrec@1 99.219 (98.022)\n",
            "Epoch: [90][165/391]\tLoss 0.0357 (0.0606)\tPrec@1 98.438 (97.948)\n",
            "Epoch: [90][220/391]\tLoss 0.0518 (0.0591)\tPrec@1 98.438 (97.999)\n",
            "Epoch: [90][275/391]\tLoss 0.0319 (0.0590)\tPrec@1 99.219 (97.999)\n",
            "Epoch: [90][330/391]\tLoss 0.0320 (0.0602)\tPrec@1 99.219 (97.939)\n",
            "Epoch: [90][385/391]\tLoss 0.0519 (0.0617)\tPrec@1 98.438 (97.865)\n",
            "Test\t  Prec@1: 87.920 (Err: 12.080 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [91][0/391]\tLoss 0.0798 (0.0798)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [91][55/391]\tLoss 0.0889 (0.0593)\tPrec@1 96.094 (97.866)\n",
            "Epoch: [91][110/391]\tLoss 0.0417 (0.0585)\tPrec@1 99.219 (97.938)\n",
            "Epoch: [91][165/391]\tLoss 0.0524 (0.0591)\tPrec@1 97.656 (97.939)\n",
            "Epoch: [91][220/391]\tLoss 0.0522 (0.0569)\tPrec@1 97.656 (97.967)\n",
            "Epoch: [91][275/391]\tLoss 0.0180 (0.0563)\tPrec@1 100.000 (98.041)\n",
            "Epoch: [91][330/391]\tLoss 0.0126 (0.0572)\tPrec@1 100.000 (98.001)\n",
            "Epoch: [91][385/391]\tLoss 0.0754 (0.0594)\tPrec@1 96.094 (97.923)\n",
            "Test\t  Prec@1: 86.650 (Err: 13.350 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [92][0/391]\tLoss 0.0930 (0.0930)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [92][55/391]\tLoss 0.0138 (0.0605)\tPrec@1 100.000 (97.977)\n",
            "Epoch: [92][110/391]\tLoss 0.0898 (0.0588)\tPrec@1 97.656 (97.980)\n",
            "Epoch: [92][165/391]\tLoss 0.0611 (0.0572)\tPrec@1 97.656 (97.957)\n",
            "Epoch: [92][220/391]\tLoss 0.0254 (0.0552)\tPrec@1 99.219 (98.013)\n",
            "Epoch: [92][275/391]\tLoss 0.0770 (0.0571)\tPrec@1 97.656 (97.939)\n",
            "Epoch: [92][330/391]\tLoss 0.0232 (0.0574)\tPrec@1 100.000 (97.968)\n",
            "Epoch: [92][385/391]\tLoss 0.0375 (0.0585)\tPrec@1 99.219 (97.940)\n",
            "Test\t  Prec@1: 88.240 (Err: 11.760 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [93][0/391]\tLoss 0.0922 (0.0922)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [93][55/391]\tLoss 0.0687 (0.0536)\tPrec@1 97.656 (97.991)\n",
            "Epoch: [93][110/391]\tLoss 0.0414 (0.0531)\tPrec@1 98.438 (98.107)\n",
            "Epoch: [93][165/391]\tLoss 0.0290 (0.0562)\tPrec@1 99.219 (97.976)\n",
            "Epoch: [93][220/391]\tLoss 0.0506 (0.0568)\tPrec@1 97.656 (97.932)\n",
            "Epoch: [93][275/391]\tLoss 0.0369 (0.0567)\tPrec@1 98.438 (97.959)\n",
            "Epoch: [93][330/391]\tLoss 0.0549 (0.0574)\tPrec@1 97.656 (97.935)\n",
            "Epoch: [93][385/391]\tLoss 0.0515 (0.0598)\tPrec@1 97.656 (97.832)\n",
            "Test\t  Prec@1: 87.680 (Err: 12.320 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [94][0/391]\tLoss 0.0332 (0.0332)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [94][55/391]\tLoss 0.0437 (0.0565)\tPrec@1 98.438 (97.879)\n",
            "Epoch: [94][110/391]\tLoss 0.0508 (0.0581)\tPrec@1 97.656 (97.832)\n",
            "Epoch: [94][165/391]\tLoss 0.0347 (0.0591)\tPrec@1 98.438 (97.821)\n",
            "Epoch: [94][220/391]\tLoss 0.0657 (0.0559)\tPrec@1 97.656 (97.996)\n",
            "Epoch: [94][275/391]\tLoss 0.0573 (0.0554)\tPrec@1 97.656 (98.038)\n",
            "Epoch: [94][330/391]\tLoss 0.0565 (0.0552)\tPrec@1 97.656 (98.053)\n",
            "Epoch: [94][385/391]\tLoss 0.0311 (0.0564)\tPrec@1 99.219 (97.986)\n",
            "Test\t  Prec@1: 88.660 (Err: 11.340 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [95][0/391]\tLoss 0.0401 (0.0401)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [95][55/391]\tLoss 0.0357 (0.0520)\tPrec@1 97.656 (98.284)\n",
            "Epoch: [95][110/391]\tLoss 0.0900 (0.0524)\tPrec@1 95.312 (98.149)\n",
            "Epoch: [95][165/391]\tLoss 0.0346 (0.0499)\tPrec@1 98.438 (98.273)\n",
            "Epoch: [95][220/391]\tLoss 0.0558 (0.0517)\tPrec@1 97.656 (98.211)\n",
            "Epoch: [95][275/391]\tLoss 0.0381 (0.0521)\tPrec@1 99.219 (98.174)\n",
            "Epoch: [95][330/391]\tLoss 0.0443 (0.0533)\tPrec@1 99.219 (98.147)\n",
            "Epoch: [95][385/391]\tLoss 0.0657 (0.0555)\tPrec@1 97.656 (98.077)\n",
            "Test\t  Prec@1: 88.510 (Err: 11.490 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [96][0/391]\tLoss 0.0411 (0.0411)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [96][55/391]\tLoss 0.0171 (0.0504)\tPrec@1 100.000 (98.270)\n",
            "Epoch: [96][110/391]\tLoss 0.0304 (0.0533)\tPrec@1 99.219 (98.135)\n",
            "Epoch: [96][165/391]\tLoss 0.0435 (0.0525)\tPrec@1 98.438 (98.226)\n",
            "Epoch: [96][220/391]\tLoss 0.0519 (0.0518)\tPrec@1 98.438 (98.197)\n",
            "Epoch: [96][275/391]\tLoss 0.1115 (0.0516)\tPrec@1 96.875 (98.220)\n",
            "Epoch: [96][330/391]\tLoss 0.0365 (0.0519)\tPrec@1 97.656 (98.185)\n",
            "Epoch: [96][385/391]\tLoss 0.0378 (0.0519)\tPrec@1 100.000 (98.195)\n",
            "Test\t  Prec@1: 88.760 (Err: 11.240 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [97][0/391]\tLoss 0.0538 (0.0538)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [97][55/391]\tLoss 0.0230 (0.0501)\tPrec@1 99.219 (98.200)\n",
            "Epoch: [97][110/391]\tLoss 0.0786 (0.0489)\tPrec@1 97.656 (98.262)\n",
            "Epoch: [97][165/391]\tLoss 0.0156 (0.0489)\tPrec@1 100.000 (98.245)\n",
            "Epoch: [97][220/391]\tLoss 0.0210 (0.0485)\tPrec@1 100.000 (98.278)\n",
            "Epoch: [97][275/391]\tLoss 0.0752 (0.0493)\tPrec@1 96.094 (98.273)\n",
            "Epoch: [97][330/391]\tLoss 0.0648 (0.0489)\tPrec@1 97.656 (98.294)\n",
            "Epoch: [97][385/391]\tLoss 0.0311 (0.0487)\tPrec@1 99.219 (98.312)\n",
            "Test\t  Prec@1: 87.820 (Err: 12.180 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [98][0/391]\tLoss 0.0381 (0.0381)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [98][55/391]\tLoss 0.0832 (0.0530)\tPrec@1 96.875 (98.410)\n",
            "Epoch: [98][110/391]\tLoss 0.0523 (0.0564)\tPrec@1 98.438 (98.198)\n",
            "Epoch: [98][165/391]\tLoss 0.0224 (0.0549)\tPrec@1 99.219 (98.169)\n",
            "Epoch: [98][220/391]\tLoss 0.0599 (0.0540)\tPrec@1 97.656 (98.151)\n",
            "Epoch: [98][275/391]\tLoss 0.0514 (0.0518)\tPrec@1 98.438 (98.228)\n",
            "Epoch: [98][330/391]\tLoss 0.0905 (0.0517)\tPrec@1 97.656 (98.220)\n",
            "Epoch: [98][385/391]\tLoss 0.1350 (0.0527)\tPrec@1 95.312 (98.185)\n",
            "Test\t  Prec@1: 88.420 (Err: 11.580 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [99][0/391]\tLoss 0.0442 (0.0442)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [99][55/391]\tLoss 0.0402 (0.0559)\tPrec@1 98.438 (97.963)\n",
            "Epoch: [99][110/391]\tLoss 0.0245 (0.0549)\tPrec@1 100.000 (97.994)\n",
            "Epoch: [99][165/391]\tLoss 0.0123 (0.0548)\tPrec@1 100.000 (98.052)\n",
            "Epoch: [99][220/391]\tLoss 0.0360 (0.0553)\tPrec@1 98.438 (98.056)\n",
            "Epoch: [99][275/391]\tLoss 0.0687 (0.0568)\tPrec@1 97.656 (97.990)\n",
            "Epoch: [99][330/391]\tLoss 0.0150 (0.0574)\tPrec@1 99.219 (97.982)\n",
            "Epoch: [99][385/391]\tLoss 0.0354 (0.0575)\tPrec@1 98.438 (97.988)\n",
            "Test\t  Prec@1: 88.180 (Err: 11.820 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [100][0/391]\tLoss 0.0250 (0.0250)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [100][55/391]\tLoss 0.0476 (0.0390)\tPrec@1 98.438 (98.549)\n",
            "Epoch: [100][110/391]\tLoss 0.0084 (0.0338)\tPrec@1 100.000 (98.825)\n",
            "Epoch: [100][165/391]\tLoss 0.0065 (0.0309)\tPrec@1 100.000 (98.955)\n",
            "Epoch: [100][220/391]\tLoss 0.0131 (0.0289)\tPrec@1 100.000 (99.070)\n",
            "Epoch: [100][275/391]\tLoss 0.0367 (0.0275)\tPrec@1 99.219 (99.148)\n",
            "Epoch: [100][330/391]\tLoss 0.0123 (0.0273)\tPrec@1 100.000 (99.155)\n",
            "Epoch: [100][385/391]\tLoss 0.0136 (0.0264)\tPrec@1 99.219 (99.190)\n",
            "Test\t  Prec@1: 89.950 (Err: 10.050 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [101][0/391]\tLoss 0.0086 (0.0086)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [101][55/391]\tLoss 0.0206 (0.0185)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [101][110/391]\tLoss 0.0338 (0.0185)\tPrec@1 98.438 (99.451)\n",
            "Epoch: [101][165/391]\tLoss 0.0247 (0.0182)\tPrec@1 99.219 (99.449)\n",
            "Epoch: [101][220/391]\tLoss 0.0082 (0.0183)\tPrec@1 100.000 (99.424)\n",
            "Epoch: [101][275/391]\tLoss 0.0396 (0.0186)\tPrec@1 97.656 (99.408)\n",
            "Epoch: [101][330/391]\tLoss 0.0107 (0.0185)\tPrec@1 100.000 (99.434)\n",
            "Epoch: [101][385/391]\tLoss 0.0163 (0.0184)\tPrec@1 100.000 (99.437)\n",
            "Test\t  Prec@1: 89.950 (Err: 10.050 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [102][0/391]\tLoss 0.0238 (0.0238)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [102][55/391]\tLoss 0.0093 (0.0158)\tPrec@1 100.000 (99.595)\n",
            "Epoch: [102][110/391]\tLoss 0.0289 (0.0159)\tPrec@1 97.656 (99.578)\n",
            "Epoch: [102][165/391]\tLoss 0.0054 (0.0165)\tPrec@1 100.000 (99.511)\n",
            "Epoch: [102][220/391]\tLoss 0.0036 (0.0171)\tPrec@1 100.000 (99.477)\n",
            "Epoch: [102][275/391]\tLoss 0.0049 (0.0167)\tPrec@1 100.000 (99.524)\n",
            "Epoch: [102][330/391]\tLoss 0.0094 (0.0166)\tPrec@1 100.000 (99.516)\n",
            "Epoch: [102][385/391]\tLoss 0.0231 (0.0165)\tPrec@1 99.219 (99.520)\n",
            "Test\t  Prec@1: 90.240 (Err: 9.760 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [103][0/391]\tLoss 0.0272 (0.0272)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [103][55/391]\tLoss 0.0072 (0.0122)\tPrec@1 100.000 (99.679)\n",
            "Epoch: [103][110/391]\tLoss 0.0037 (0.0143)\tPrec@1 100.000 (99.627)\n",
            "Epoch: [103][165/391]\tLoss 0.0137 (0.0143)\tPrec@1 100.000 (99.638)\n",
            "Epoch: [103][220/391]\tLoss 0.0152 (0.0143)\tPrec@1 99.219 (99.618)\n",
            "Epoch: [103][275/391]\tLoss 0.0160 (0.0142)\tPrec@1 100.000 (99.624)\n",
            "Epoch: [103][330/391]\tLoss 0.0085 (0.0145)\tPrec@1 100.000 (99.618)\n",
            "Epoch: [103][385/391]\tLoss 0.0053 (0.0144)\tPrec@1 100.000 (99.622)\n",
            "Test\t  Prec@1: 90.450 (Err: 9.550 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [104][0/391]\tLoss 0.0051 (0.0051)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [104][55/391]\tLoss 0.0045 (0.0130)\tPrec@1 100.000 (99.693)\n",
            "Epoch: [104][110/391]\tLoss 0.0310 (0.0131)\tPrec@1 99.219 (99.634)\n",
            "Epoch: [104][165/391]\tLoss 0.0115 (0.0132)\tPrec@1 100.000 (99.638)\n",
            "Epoch: [104][220/391]\tLoss 0.0086 (0.0128)\tPrec@1 100.000 (99.654)\n",
            "Epoch: [104][275/391]\tLoss 0.0091 (0.0126)\tPrec@1 100.000 (99.677)\n",
            "Epoch: [104][330/391]\tLoss 0.0233 (0.0127)\tPrec@1 98.438 (99.681)\n",
            "Epoch: [104][385/391]\tLoss 0.0094 (0.0129)\tPrec@1 100.000 (99.672)\n",
            "Test\t  Prec@1: 90.390 (Err: 9.610 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [105][0/391]\tLoss 0.0065 (0.0065)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [105][55/391]\tLoss 0.0047 (0.0124)\tPrec@1 100.000 (99.707)\n",
            "Epoch: [105][110/391]\tLoss 0.0346 (0.0120)\tPrec@1 99.219 (99.726)\n",
            "Epoch: [105][165/391]\tLoss 0.0203 (0.0120)\tPrec@1 99.219 (99.736)\n",
            "Epoch: [105][220/391]\tLoss 0.0226 (0.0123)\tPrec@1 98.438 (99.731)\n",
            "Epoch: [105][275/391]\tLoss 0.0080 (0.0121)\tPrec@1 100.000 (99.728)\n",
            "Epoch: [105][330/391]\tLoss 0.0109 (0.0123)\tPrec@1 100.000 (99.705)\n",
            "Epoch: [105][385/391]\tLoss 0.0127 (0.0121)\tPrec@1 99.219 (99.707)\n",
            "Test\t  Prec@1: 90.290 (Err: 9.710 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [106][0/391]\tLoss 0.0039 (0.0039)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [106][55/391]\tLoss 0.0083 (0.0094)\tPrec@1 100.000 (99.819)\n",
            "Epoch: [106][110/391]\tLoss 0.0092 (0.0105)\tPrec@1 100.000 (99.747)\n",
            "Epoch: [106][165/391]\tLoss 0.0219 (0.0102)\tPrec@1 99.219 (99.769)\n",
            "Epoch: [106][220/391]\tLoss 0.0273 (0.0108)\tPrec@1 99.219 (99.742)\n",
            "Epoch: [106][275/391]\tLoss 0.0075 (0.0109)\tPrec@1 100.000 (99.740)\n",
            "Epoch: [106][330/391]\tLoss 0.0342 (0.0110)\tPrec@1 99.219 (99.724)\n",
            "Epoch: [106][385/391]\tLoss 0.0081 (0.0113)\tPrec@1 100.000 (99.717)\n",
            "Test\t  Prec@1: 90.470 (Err: 9.530 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [107][0/391]\tLoss 0.0224 (0.0224)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [107][55/391]\tLoss 0.0132 (0.0112)\tPrec@1 100.000 (99.721)\n",
            "Epoch: [107][110/391]\tLoss 0.0027 (0.0114)\tPrec@1 100.000 (99.711)\n",
            "Epoch: [107][165/391]\tLoss 0.0088 (0.0112)\tPrec@1 100.000 (99.722)\n",
            "Epoch: [107][220/391]\tLoss 0.0062 (0.0110)\tPrec@1 100.000 (99.724)\n",
            "Epoch: [107][275/391]\tLoss 0.0054 (0.0109)\tPrec@1 100.000 (99.723)\n",
            "Epoch: [107][330/391]\tLoss 0.0068 (0.0109)\tPrec@1 100.000 (99.731)\n",
            "Epoch: [107][385/391]\tLoss 0.0127 (0.0108)\tPrec@1 99.219 (99.735)\n",
            "Test\t  Prec@1: 90.540 (Err: 9.460 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [108][0/391]\tLoss 0.0049 (0.0049)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [108][55/391]\tLoss 0.0093 (0.0087)\tPrec@1 100.000 (99.819)\n",
            "Epoch: [108][110/391]\tLoss 0.0110 (0.0102)\tPrec@1 100.000 (99.768)\n",
            "Epoch: [108][165/391]\tLoss 0.0109 (0.0100)\tPrec@1 99.219 (99.779)\n",
            "Epoch: [108][220/391]\tLoss 0.0052 (0.0104)\tPrec@1 100.000 (99.753)\n",
            "Epoch: [108][275/391]\tLoss 0.0057 (0.0103)\tPrec@1 100.000 (99.748)\n",
            "Epoch: [108][330/391]\tLoss 0.0044 (0.0104)\tPrec@1 100.000 (99.750)\n",
            "Epoch: [108][385/391]\tLoss 0.0116 (0.0105)\tPrec@1 99.219 (99.741)\n",
            "Test\t  Prec@1: 90.440 (Err: 9.560 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [109][0/391]\tLoss 0.0043 (0.0043)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [109][55/391]\tLoss 0.0128 (0.0112)\tPrec@1 100.000 (99.707)\n",
            "Epoch: [109][110/391]\tLoss 0.0061 (0.0100)\tPrec@1 100.000 (99.803)\n",
            "Epoch: [109][165/391]\tLoss 0.0058 (0.0099)\tPrec@1 100.000 (99.798)\n",
            "Epoch: [109][220/391]\tLoss 0.0043 (0.0099)\tPrec@1 100.000 (99.791)\n",
            "Epoch: [109][275/391]\tLoss 0.0176 (0.0097)\tPrec@1 98.438 (99.796)\n",
            "Epoch: [109][330/391]\tLoss 0.0094 (0.0097)\tPrec@1 100.000 (99.790)\n",
            "Epoch: [109][385/391]\tLoss 0.0038 (0.0095)\tPrec@1 100.000 (99.796)\n",
            "Test\t  Prec@1: 90.660 (Err: 9.340 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [110][0/391]\tLoss 0.0071 (0.0071)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [110][55/391]\tLoss 0.0102 (0.0066)\tPrec@1 100.000 (99.944)\n",
            "Epoch: [110][110/391]\tLoss 0.0082 (0.0079)\tPrec@1 100.000 (99.873)\n",
            "Epoch: [110][165/391]\tLoss 0.0068 (0.0081)\tPrec@1 100.000 (99.859)\n",
            "Epoch: [110][220/391]\tLoss 0.0023 (0.0082)\tPrec@1 100.000 (99.852)\n",
            "Epoch: [110][275/391]\tLoss 0.0209 (0.0084)\tPrec@1 99.219 (99.844)\n",
            "Epoch: [110][330/391]\tLoss 0.0055 (0.0085)\tPrec@1 100.000 (99.844)\n",
            "Epoch: [110][385/391]\tLoss 0.0027 (0.0083)\tPrec@1 100.000 (99.844)\n",
            "Test\t  Prec@1: 90.520 (Err: 9.480 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [111][0/391]\tLoss 0.0092 (0.0092)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [111][55/391]\tLoss 0.0105 (0.0089)\tPrec@1 100.000 (99.777)\n",
            "Epoch: [111][110/391]\tLoss 0.0061 (0.0087)\tPrec@1 100.000 (99.782)\n",
            "Epoch: [111][165/391]\tLoss 0.0093 (0.0091)\tPrec@1 99.219 (99.765)\n",
            "Epoch: [111][220/391]\tLoss 0.0069 (0.0088)\tPrec@1 100.000 (99.777)\n",
            "Epoch: [111][275/391]\tLoss 0.0123 (0.0092)\tPrec@1 99.219 (99.774)\n",
            "Epoch: [111][330/391]\tLoss 0.0060 (0.0093)\tPrec@1 100.000 (99.755)\n",
            "Epoch: [111][385/391]\tLoss 0.0056 (0.0091)\tPrec@1 100.000 (99.761)\n",
            "Test\t  Prec@1: 90.480 (Err: 9.520 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [112][0/391]\tLoss 0.0036 (0.0036)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [112][55/391]\tLoss 0.0116 (0.0071)\tPrec@1 100.000 (99.860)\n",
            "Epoch: [112][110/391]\tLoss 0.0017 (0.0071)\tPrec@1 100.000 (99.873)\n",
            "Epoch: [112][165/391]\tLoss 0.0069 (0.0074)\tPrec@1 100.000 (99.854)\n",
            "Epoch: [112][220/391]\tLoss 0.0161 (0.0075)\tPrec@1 100.000 (99.852)\n",
            "Epoch: [112][275/391]\tLoss 0.0111 (0.0079)\tPrec@1 99.219 (99.827)\n",
            "Epoch: [112][330/391]\tLoss 0.0316 (0.0078)\tPrec@1 99.219 (99.825)\n",
            "Epoch: [112][385/391]\tLoss 0.0029 (0.0079)\tPrec@1 100.000 (99.832)\n",
            "Test\t  Prec@1: 90.510 (Err: 9.490 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [113][0/391]\tLoss 0.0071 (0.0071)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [113][55/391]\tLoss 0.0057 (0.0095)\tPrec@1 100.000 (99.693)\n",
            "Epoch: [113][110/391]\tLoss 0.0049 (0.0080)\tPrec@1 100.000 (99.782)\n",
            "Epoch: [113][165/391]\tLoss 0.0109 (0.0077)\tPrec@1 100.000 (99.807)\n",
            "Epoch: [113][220/391]\tLoss 0.0041 (0.0079)\tPrec@1 100.000 (99.813)\n",
            "Epoch: [113][275/391]\tLoss 0.0127 (0.0080)\tPrec@1 100.000 (99.810)\n",
            "Epoch: [113][330/391]\tLoss 0.0071 (0.0080)\tPrec@1 100.000 (99.821)\n",
            "Epoch: [113][385/391]\tLoss 0.0018 (0.0079)\tPrec@1 100.000 (99.822)\n",
            "Test\t  Prec@1: 90.420 (Err: 9.580 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [114][0/391]\tLoss 0.0063 (0.0063)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [114][55/391]\tLoss 0.0024 (0.0074)\tPrec@1 100.000 (99.819)\n",
            "Epoch: [114][110/391]\tLoss 0.0057 (0.0075)\tPrec@1 100.000 (99.831)\n",
            "Epoch: [114][165/391]\tLoss 0.0007 (0.0070)\tPrec@1 100.000 (99.859)\n",
            "Epoch: [114][220/391]\tLoss 0.0079 (0.0071)\tPrec@1 100.000 (99.848)\n",
            "Epoch: [114][275/391]\tLoss 0.0054 (0.0073)\tPrec@1 100.000 (99.847)\n",
            "Epoch: [114][330/391]\tLoss 0.0585 (0.0073)\tPrec@1 99.219 (99.854)\n",
            "Epoch: [114][385/391]\tLoss 0.0108 (0.0074)\tPrec@1 99.219 (99.844)\n",
            "Test\t  Prec@1: 90.460 (Err: 9.540 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [115][0/391]\tLoss 0.0043 (0.0043)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [115][55/391]\tLoss 0.0020 (0.0083)\tPrec@1 100.000 (99.805)\n",
            "Epoch: [115][110/391]\tLoss 0.0042 (0.0076)\tPrec@1 100.000 (99.845)\n",
            "Epoch: [115][165/391]\tLoss 0.0052 (0.0075)\tPrec@1 100.000 (99.840)\n",
            "Epoch: [115][220/391]\tLoss 0.0112 (0.0073)\tPrec@1 100.000 (99.848)\n",
            "Epoch: [115][275/391]\tLoss 0.0300 (0.0074)\tPrec@1 99.219 (99.858)\n",
            "Epoch: [115][330/391]\tLoss 0.0050 (0.0075)\tPrec@1 100.000 (99.856)\n",
            "Epoch: [115][385/391]\tLoss 0.0052 (0.0076)\tPrec@1 100.000 (99.854)\n",
            "Test\t  Prec@1: 90.380 (Err: 9.620 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [116][0/391]\tLoss 0.0168 (0.0168)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [116][55/391]\tLoss 0.0059 (0.0074)\tPrec@1 100.000 (99.930)\n",
            "Epoch: [116][110/391]\tLoss 0.0414 (0.0081)\tPrec@1 98.438 (99.845)\n",
            "Epoch: [116][165/391]\tLoss 0.0067 (0.0077)\tPrec@1 100.000 (99.831)\n",
            "Epoch: [116][220/391]\tLoss 0.0408 (0.0080)\tPrec@1 99.219 (99.827)\n",
            "Epoch: [116][275/391]\tLoss 0.0024 (0.0080)\tPrec@1 100.000 (99.825)\n",
            "Epoch: [116][330/391]\tLoss 0.0140 (0.0078)\tPrec@1 100.000 (99.842)\n",
            "Epoch: [116][385/391]\tLoss 0.0043 (0.0077)\tPrec@1 100.000 (99.844)\n",
            "Test\t  Prec@1: 90.680 (Err: 9.320 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [117][0/391]\tLoss 0.0052 (0.0052)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [117][55/391]\tLoss 0.0075 (0.0066)\tPrec@1 100.000 (99.902)\n",
            "Epoch: [117][110/391]\tLoss 0.0106 (0.0069)\tPrec@1 100.000 (99.880)\n",
            "Epoch: [117][165/391]\tLoss 0.0024 (0.0075)\tPrec@1 100.000 (99.849)\n",
            "Epoch: [117][220/391]\tLoss 0.0093 (0.0076)\tPrec@1 100.000 (99.848)\n",
            "Epoch: [117][275/391]\tLoss 0.0038 (0.0074)\tPrec@1 100.000 (99.850)\n",
            "Epoch: [117][330/391]\tLoss 0.0060 (0.0072)\tPrec@1 100.000 (99.854)\n",
            "Epoch: [117][385/391]\tLoss 0.0113 (0.0070)\tPrec@1 100.000 (99.866)\n",
            "Test\t  Prec@1: 90.630 (Err: 9.370 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [118][0/391]\tLoss 0.0045 (0.0045)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [118][55/391]\tLoss 0.0025 (0.0063)\tPrec@1 100.000 (99.860)\n",
            "Epoch: [118][110/391]\tLoss 0.0045 (0.0060)\tPrec@1 100.000 (99.894)\n",
            "Epoch: [118][165/391]\tLoss 0.0090 (0.0063)\tPrec@1 100.000 (99.878)\n",
            "Epoch: [118][220/391]\tLoss 0.0060 (0.0066)\tPrec@1 100.000 (99.855)\n",
            "Epoch: [118][275/391]\tLoss 0.0024 (0.0064)\tPrec@1 100.000 (99.875)\n",
            "Epoch: [118][330/391]\tLoss 0.0055 (0.0064)\tPrec@1 100.000 (99.873)\n",
            "Epoch: [118][385/391]\tLoss 0.0031 (0.0064)\tPrec@1 100.000 (99.870)\n",
            "Test\t  Prec@1: 90.600 (Err: 9.400 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [119][0/391]\tLoss 0.0083 (0.0083)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [119][55/391]\tLoss 0.0268 (0.0071)\tPrec@1 99.219 (99.847)\n",
            "Epoch: [119][110/391]\tLoss 0.0051 (0.0065)\tPrec@1 100.000 (99.880)\n",
            "Epoch: [119][165/391]\tLoss 0.0056 (0.0067)\tPrec@1 100.000 (99.878)\n",
            "Epoch: [119][220/391]\tLoss 0.0404 (0.0071)\tPrec@1 98.438 (99.852)\n",
            "Epoch: [119][275/391]\tLoss 0.0039 (0.0073)\tPrec@1 100.000 (99.847)\n",
            "Epoch: [119][330/391]\tLoss 0.0029 (0.0073)\tPrec@1 100.000 (99.851)\n",
            "Epoch: [119][385/391]\tLoss 0.0035 (0.0071)\tPrec@1 100.000 (99.852)\n",
            "Test\t  Prec@1: 90.730 (Err: 9.270 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [120][0/391]\tLoss 0.0015 (0.0015)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [120][55/391]\tLoss 0.0036 (0.0055)\tPrec@1 100.000 (99.916)\n",
            "Epoch: [120][110/391]\tLoss 0.0030 (0.0058)\tPrec@1 100.000 (99.901)\n",
            "Epoch: [120][165/391]\tLoss 0.0052 (0.0064)\tPrec@1 100.000 (99.878)\n",
            "Epoch: [120][220/391]\tLoss 0.0066 (0.0064)\tPrec@1 100.000 (99.883)\n",
            "Epoch: [120][275/391]\tLoss 0.0098 (0.0064)\tPrec@1 100.000 (99.878)\n",
            "Epoch: [120][330/391]\tLoss 0.0066 (0.0065)\tPrec@1 100.000 (99.863)\n",
            "Epoch: [120][385/391]\tLoss 0.0029 (0.0066)\tPrec@1 100.000 (99.860)\n",
            "Test\t  Prec@1: 90.530 (Err: 9.470 )\n",
            "\n",
            "Training resnet110 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [121][0/391]\tLoss 0.0045 (0.0045)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [121][55/391]\tLoss 0.0125 (0.0063)\tPrec@1 99.219 (99.916)\n",
            "Epoch: [121][110/391]\tLoss 0.0018 (0.0064)\tPrec@1 100.000 (99.894)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}